{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9944fcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cec77a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_csv('./data1.csv', header=None)\n",
    "labels = pd.read_csv('./label1.csv', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9254a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.446891</td>\n",
       "      <td>-0.013397</td>\n",
       "      <td>0.232645</td>\n",
       "      <td>2.156649</td>\n",
       "      <td>1.652923</td>\n",
       "      <td>-0.210531</td>\n",
       "      <td>-0.662227</td>\n",
       "      <td>0.705144</td>\n",
       "      <td>1.434684</td>\n",
       "      <td>-0.445750</td>\n",
       "      <td>-0.178501</td>\n",
       "      <td>-0.905503</td>\n",
       "      <td>-0.594099</td>\n",
       "      <td>-0.728586</td>\n",
       "      <td>-0.627002</td>\n",
       "      <td>-0.240520</td>\n",
       "      <td>0.131696</td>\n",
       "      <td>-0.945878</td>\n",
       "      <td>1.840922</td>\n",
       "      <td>-0.279024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.243747</td>\n",
       "      <td>-1.144175</td>\n",
       "      <td>-0.622214</td>\n",
       "      <td>-0.661979</td>\n",
       "      <td>-0.373315</td>\n",
       "      <td>0.520313</td>\n",
       "      <td>-1.307954</td>\n",
       "      <td>1.639481</td>\n",
       "      <td>-2.838816</td>\n",
       "      <td>0.331636</td>\n",
       "      <td>0.100261</td>\n",
       "      <td>-1.173067</td>\n",
       "      <td>-0.079774</td>\n",
       "      <td>2.194899</td>\n",
       "      <td>0.242839</td>\n",
       "      <td>0.412164</td>\n",
       "      <td>-0.328714</td>\n",
       "      <td>0.902931</td>\n",
       "      <td>-1.418728</td>\n",
       "      <td>-0.290139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.417799</td>\n",
       "      <td>-0.088833</td>\n",
       "      <td>-0.647181</td>\n",
       "      <td>-1.141497</td>\n",
       "      <td>-1.321100</td>\n",
       "      <td>0.686798</td>\n",
       "      <td>-1.045569</td>\n",
       "      <td>-0.376153</td>\n",
       "      <td>-2.272447</td>\n",
       "      <td>-0.669733</td>\n",
       "      <td>-1.070732</td>\n",
       "      <td>1.091993</td>\n",
       "      <td>0.692293</td>\n",
       "      <td>0.937617</td>\n",
       "      <td>0.262142</td>\n",
       "      <td>0.399238</td>\n",
       "      <td>0.614614</td>\n",
       "      <td>1.209465</td>\n",
       "      <td>-1.526386</td>\n",
       "      <td>0.337771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.656784</td>\n",
       "      <td>0.381201</td>\n",
       "      <td>-1.039011</td>\n",
       "      <td>1.285315</td>\n",
       "      <td>0.808324</td>\n",
       "      <td>1.428519</td>\n",
       "      <td>-1.144367</td>\n",
       "      <td>-0.609958</td>\n",
       "      <td>0.983676</td>\n",
       "      <td>0.060917</td>\n",
       "      <td>0.471647</td>\n",
       "      <td>-0.594279</td>\n",
       "      <td>2.104027</td>\n",
       "      <td>-1.113597</td>\n",
       "      <td>-0.188799</td>\n",
       "      <td>1.226253</td>\n",
       "      <td>-1.067763</td>\n",
       "      <td>-0.350275</td>\n",
       "      <td>-0.746426</td>\n",
       "      <td>-1.258696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.098520</td>\n",
       "      <td>1.706769</td>\n",
       "      <td>-1.030370</td>\n",
       "      <td>2.001009</td>\n",
       "      <td>2.751551</td>\n",
       "      <td>1.202229</td>\n",
       "      <td>-0.941066</td>\n",
       "      <td>1.965282</td>\n",
       "      <td>2.979131</td>\n",
       "      <td>1.402483</td>\n",
       "      <td>-1.432987</td>\n",
       "      <td>0.400322</td>\n",
       "      <td>-0.839851</td>\n",
       "      <td>-1.580974</td>\n",
       "      <td>-0.659587</td>\n",
       "      <td>2.292688</td>\n",
       "      <td>0.494188</td>\n",
       "      <td>-1.583187</td>\n",
       "      <td>-2.468254</td>\n",
       "      <td>-0.469648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>0.146324</td>\n",
       "      <td>-0.618555</td>\n",
       "      <td>0.547413</td>\n",
       "      <td>0.851301</td>\n",
       "      <td>0.838880</td>\n",
       "      <td>-1.248402</td>\n",
       "      <td>0.698199</td>\n",
       "      <td>-0.545033</td>\n",
       "      <td>0.994348</td>\n",
       "      <td>-0.054620</td>\n",
       "      <td>0.581239</td>\n",
       "      <td>-1.842074</td>\n",
       "      <td>-0.552879</td>\n",
       "      <td>-0.102096</td>\n",
       "      <td>0.307538</td>\n",
       "      <td>-1.267832</td>\n",
       "      <td>-0.165875</td>\n",
       "      <td>-0.717397</td>\n",
       "      <td>-0.808588</td>\n",
       "      <td>0.796346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>-2.227923</td>\n",
       "      <td>-0.327196</td>\n",
       "      <td>1.258849</td>\n",
       "      <td>-0.945254</td>\n",
       "      <td>3.925262</td>\n",
       "      <td>0.316686</td>\n",
       "      <td>-1.061159</td>\n",
       "      <td>1.846706</td>\n",
       "      <td>3.692157</td>\n",
       "      <td>0.698035</td>\n",
       "      <td>-0.184239</td>\n",
       "      <td>-0.546036</td>\n",
       "      <td>-0.457919</td>\n",
       "      <td>-1.030581</td>\n",
       "      <td>2.406014</td>\n",
       "      <td>-0.978943</td>\n",
       "      <td>-0.086879</td>\n",
       "      <td>-1.964453</td>\n",
       "      <td>-0.875574</td>\n",
       "      <td>1.535930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>-0.800405</td>\n",
       "      <td>1.477997</td>\n",
       "      <td>0.090057</td>\n",
       "      <td>-0.973997</td>\n",
       "      <td>-0.204970</td>\n",
       "      <td>0.224539</td>\n",
       "      <td>0.302265</td>\n",
       "      <td>0.361146</td>\n",
       "      <td>-1.906636</td>\n",
       "      <td>1.381875</td>\n",
       "      <td>0.034366</td>\n",
       "      <td>1.690199</td>\n",
       "      <td>-0.901569</td>\n",
       "      <td>2.183842</td>\n",
       "      <td>0.831008</td>\n",
       "      <td>0.036308</td>\n",
       "      <td>0.516088</td>\n",
       "      <td>0.378670</td>\n",
       "      <td>0.110968</td>\n",
       "      <td>-0.176363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>-1.217344</td>\n",
       "      <td>-0.785401</td>\n",
       "      <td>-0.413781</td>\n",
       "      <td>-0.335324</td>\n",
       "      <td>0.181671</td>\n",
       "      <td>-1.685733</td>\n",
       "      <td>-0.437981</td>\n",
       "      <td>-1.176626</td>\n",
       "      <td>-1.434436</td>\n",
       "      <td>-0.880986</td>\n",
       "      <td>2.117703</td>\n",
       "      <td>-2.136842</td>\n",
       "      <td>-1.185261</td>\n",
       "      <td>1.547704</td>\n",
       "      <td>1.174712</td>\n",
       "      <td>-0.429117</td>\n",
       "      <td>0.132068</td>\n",
       "      <td>0.249277</td>\n",
       "      <td>-0.941496</td>\n",
       "      <td>1.661661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>1.601005</td>\n",
       "      <td>-0.890398</td>\n",
       "      <td>-1.425049</td>\n",
       "      <td>0.887501</td>\n",
       "      <td>0.167205</td>\n",
       "      <td>-0.395042</td>\n",
       "      <td>-0.512554</td>\n",
       "      <td>0.050777</td>\n",
       "      <td>-0.034945</td>\n",
       "      <td>-0.289219</td>\n",
       "      <td>0.639189</td>\n",
       "      <td>0.981751</td>\n",
       "      <td>-0.110296</td>\n",
       "      <td>-0.187214</td>\n",
       "      <td>-0.226811</td>\n",
       "      <td>-0.547772</td>\n",
       "      <td>-0.006706</td>\n",
       "      <td>-0.023773</td>\n",
       "      <td>1.220809</td>\n",
       "      <td>1.489330</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             0         1         2         3         4         5         6   \\\n",
       "0     -0.446891 -0.013397  0.232645  2.156649  1.652923 -0.210531 -0.662227   \n",
       "1      0.243747 -1.144175 -0.622214 -0.661979 -0.373315  0.520313 -1.307954   \n",
       "2     -1.417799 -0.088833 -0.647181 -1.141497 -1.321100  0.686798 -1.045569   \n",
       "3      0.656784  0.381201 -1.039011  1.285315  0.808324  1.428519 -1.144367   \n",
       "4      1.098520  1.706769 -1.030370  2.001009  2.751551  1.202229 -0.941066   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19995  0.146324 -0.618555  0.547413  0.851301  0.838880 -1.248402  0.698199   \n",
       "19996 -2.227923 -0.327196  1.258849 -0.945254  3.925262  0.316686 -1.061159   \n",
       "19997 -0.800405  1.477997  0.090057 -0.973997 -0.204970  0.224539  0.302265   \n",
       "19998 -1.217344 -0.785401 -0.413781 -0.335324  0.181671 -1.685733 -0.437981   \n",
       "19999  1.601005 -0.890398 -1.425049  0.887501  0.167205 -0.395042 -0.512554   \n",
       "\n",
       "             7         8         9         10        11        12        13  \\\n",
       "0      0.705144  1.434684 -0.445750 -0.178501 -0.905503 -0.594099 -0.728586   \n",
       "1      1.639481 -2.838816  0.331636  0.100261 -1.173067 -0.079774  2.194899   \n",
       "2     -0.376153 -2.272447 -0.669733 -1.070732  1.091993  0.692293  0.937617   \n",
       "3     -0.609958  0.983676  0.060917  0.471647 -0.594279  2.104027 -1.113597   \n",
       "4      1.965282  2.979131  1.402483 -1.432987  0.400322 -0.839851 -1.580974   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19995 -0.545033  0.994348 -0.054620  0.581239 -1.842074 -0.552879 -0.102096   \n",
       "19996  1.846706  3.692157  0.698035 -0.184239 -0.546036 -0.457919 -1.030581   \n",
       "19997  0.361146 -1.906636  1.381875  0.034366  1.690199 -0.901569  2.183842   \n",
       "19998 -1.176626 -1.434436 -0.880986  2.117703 -2.136842 -1.185261  1.547704   \n",
       "19999  0.050777 -0.034945 -0.289219  0.639189  0.981751 -0.110296 -0.187214   \n",
       "\n",
       "             14        15        16        17        18        19  \n",
       "0     -0.627002 -0.240520  0.131696 -0.945878  1.840922 -0.279024  \n",
       "1      0.242839  0.412164 -0.328714  0.902931 -1.418728 -0.290139  \n",
       "2      0.262142  0.399238  0.614614  1.209465 -1.526386  0.337771  \n",
       "3     -0.188799  1.226253 -1.067763 -0.350275 -0.746426 -1.258696  \n",
       "4     -0.659587  2.292688  0.494188 -1.583187 -2.468254 -0.469648  \n",
       "...         ...       ...       ...       ...       ...       ...  \n",
       "19995  0.307538 -1.267832 -0.165875 -0.717397 -0.808588  0.796346  \n",
       "19996  2.406014 -0.978943 -0.086879 -1.964453 -0.875574  1.535930  \n",
       "19997  0.831008  0.036308  0.516088  0.378670  0.110968 -0.176363  \n",
       "19998  1.174712 -0.429117  0.132068  0.249277 -0.941496  1.661661  \n",
       "19999 -0.226811 -0.547772 -0.006706 -0.023773  1.220809  1.489330  \n",
       "\n",
       "[20000 rows x 20 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b636a70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19995</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19996</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19997</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19998</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19999</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0\n",
       "0      3\n",
       "1      2\n",
       "2      2\n",
       "3      3\n",
       "4      3\n",
       "...   ..\n",
       "19995  3\n",
       "19996  1\n",
       "19997  2\n",
       "19998  2\n",
       "19999  0\n",
       "\n",
       "[20000 rows x 1 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9911235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "npData = data.values\n",
    "npLabels = labels.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "950dbb3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.44689111 -0.01339702  0.23264493 ... -0.94587809  1.84092189\n",
      "  -0.27902404]\n",
      " [ 0.24374694 -1.14417545 -0.62221358 ...  0.90293107 -1.41872787\n",
      "  -0.29013935]\n",
      " [-1.41779879 -0.0888327  -0.647181   ...  1.20946469 -1.52638614\n",
      "   0.33777109]\n",
      " ...\n",
      " [-0.80040466  1.47799654  0.0900574  ...  0.37866958  0.11096771\n",
      "  -0.17636265]\n",
      " [-1.21734449 -0.78540102 -0.41378073 ...  0.24927734 -0.94149613\n",
      "   1.66166061]\n",
      " [ 1.60100517 -0.89039824 -1.42504908 ... -0.02377265  1.2208092\n",
      "   1.48933029]]\n",
      "[[3]\n",
      " [2]\n",
      " [2]\n",
      " ...\n",
      " [2]\n",
      " [2]\n",
      " [0]]\n"
     ]
    }
   ],
   "source": [
    "print(npData)\n",
    "print(\"this \")\n",
    "print(npLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d93e7692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "6.873800762086958\n"
     ]
    }
   ],
   "source": [
    "print(np.max(npLabels))\n",
    "print(np.max(npData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ec17d46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(npLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3305abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(20000, 1, 5)\n"
     ]
    }
   ],
   "source": [
    "num=np.max(npLabels)+1\n",
    "oneHot=np.eye(num)[npLabels]\n",
    "print(oneHot.ndim)\n",
    "print(oneHot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "993467c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 1. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "oneHotRe = np.reshape(oneHot,[20000,5])\n",
    "print(oneHotRe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3e8f5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData, testData, trainLabel, testLabel = train_test_split(npData,oneHotRe,train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a7dd5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 20) (4000, 20) (16000, 5) (4000, 5)\n"
     ]
    }
   ],
   "source": [
    "print(trainData.shape,testData.shape,trainLabel.shape,testLabel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c1e82412",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self,InputL,hiddenLayers,OutputL):\n",
    "\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "\n",
    "        self.weights.append(np.random.normal(0,1/np.sqrt(InputL),size=(InputL,hiddenLayers[0])))\n",
    "        self.bias.append(np.zeros((hiddenLayers[0],)))\n",
    "\n",
    "        for i in range(1,len(hiddenLayers)):\n",
    "            self.weights.append(np.random.normal(0,1/np.sqrt(hiddenLayers[i-1]),size=(hiddenLayers[i-1],hiddenLayers[i])))\n",
    "            self.bias.append(np.zeros((hiddenLayers[i],)))\n",
    "\n",
    "        self.weights.append(np.random.normal(0,1/np.sqrt(hiddenLayers[-1]),size=(hiddenLayers[-1],OutputL)))\n",
    "        self.bias.append(np.zeros((OutputL,)))\n",
    "\n",
    "    def sigmoidActivation(self,Z):\n",
    "        return 1.0/(1.0+np.exp(-Z))\n",
    "\n",
    "    def softmaxActivation(self,Z):\n",
    "        temp = np.exp(Z)\n",
    "        return temp/np.sum(temp,axis = 1,keepdims=True)\n",
    "\n",
    "    def fit(self,X,y,lr = 0.01,epochs = 1000):\n",
    "        self.backward(X,y,lr,epochs)\n",
    "\n",
    "    def predict(self,X):\n",
    "        ypred = self.forward(X)\n",
    "        ypred[ypred >= 0.5] = 1.0\n",
    "        ypred[ypred < 0.5] = 0.0\n",
    "        return ypred\n",
    "\n",
    "    def Acc(self,y,t,size):\n",
    "        maxData = np.argmax(y,axis = 1)\n",
    "        maxLabel = np.argmax(t,axis = 1)\n",
    "        compare = np.equal(maxData,maxLabel)\n",
    "    \n",
    "        count = np.sum(compare)\n",
    "        return (count/size)*100\n",
    "\n",
    "    def costFn(self,y,t):\n",
    "        return -(t*np.log(y))\n",
    "        \n",
    "    def forward(self,X):\n",
    "        \n",
    "        a = X\n",
    "        self.output = [a]\n",
    "        for i in range(len(self.weights)-1):\n",
    "            z = a@self.weights[i] + self.bias[i]\n",
    "            a = self.sigmoidActivation(z)\n",
    "            self.output.append(a)\n",
    "\n",
    "        z = a@self.weights[-1] + self.bias[-1]\n",
    "        a = self.softmaxActivation(z)\n",
    "        self.output.append(a)\n",
    "            \n",
    "        return a\n",
    "\n",
    "    def derA(self,Z):\n",
    "        return Z*(1-Z)\n",
    "\n",
    "    def backward(self,X,y,lr = 0.01,epochs = 100):\n",
    "        losses = []\n",
    "        for e in range(epochs):\n",
    "            ypred = self.forward(X)\n",
    "    \n",
    "            delta = [ypred - y]\n",
    "            gradw = [self.output[-2].T@delta[-1]]\n",
    "\n",
    "            # print(delta)\n",
    "\n",
    "            for i in range(len(self.weights)-1,0,-1):\n",
    "                delta.append((delta[-1]@self.weights[i].T)*self.derA(self.output[i]))\n",
    "                gradw.append(self.output[i-1].T@delta[-1])\n",
    "\n",
    "\n",
    "            wn = len(self.weights)-1\n",
    "            \n",
    "            for i in range(len(gradw)):\n",
    "                self.weights[wn] = self.weights[wn] - lr*gradw[i]\n",
    "                self.bias[wn] = self.bias[wn] - lr*np.sum(delta[i],axis = 0)\n",
    "                wn -= 1\n",
    "\n",
    "            loss = np.sum(self.costFn(ypred,y))/ypred.shape[0]\n",
    "            print(\"Epoch:\",e,\"Train Loss:\",loss,\"Train Accuracy:\",self.Acc(ypred,y,y.shape[0]))\n",
    "            losses.append(loss)\n",
    "            \n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(losses, color='blue', label='Training Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d757be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5810e043",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MLP(trainData.shape[1],[40,30,60],trainLabel.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a97db0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 1.6209665539080649 Train Accuracy: 23.78125\n",
      "Epoch: 1 Train Loss: 1.8921245278728331 Train Accuracy: 21.04375\n",
      "Epoch: 2 Train Loss: 3.6503308546716435 Train Accuracy: 23.49375\n",
      "Epoch: 3 Train Loss: 4.278706393116245 Train Accuracy: 20.181250000000002\n",
      "Epoch: 4 Train Loss: 3.971565187770783 Train Accuracy: 20.09375\n",
      "Epoch: 5 Train Loss: 2.4785098432176818 Train Accuracy: 20.0625\n",
      "Epoch: 6 Train Loss: 1.7640025945861995 Train Accuracy: 19.875\n",
      "Epoch: 7 Train Loss: 1.6292295823517529 Train Accuracy: 28.199999999999996\n",
      "Epoch: 8 Train Loss: 1.6045073404714814 Train Accuracy: 28.29375\n",
      "Epoch: 9 Train Loss: 1.6011603557023075 Train Accuracy: 28.60625\n",
      "Epoch: 10 Train Loss: 1.6003857397445747 Train Accuracy: 34.78125\n",
      "Epoch: 11 Train Loss: 1.59974829772857 Train Accuracy: 37.39375\n",
      "Epoch: 12 Train Loss: 1.5990853161053857 Train Accuracy: 38.15\n",
      "Epoch: 13 Train Loss: 1.5983857836100162 Train Accuracy: 38.68125\n",
      "Epoch: 14 Train Loss: 1.597645789857898 Train Accuracy: 39.19375\n",
      "Epoch: 15 Train Loss: 1.596861383108879 Train Accuracy: 39.95625\n",
      "Epoch: 16 Train Loss: 1.5960282021591659 Train Accuracy: 40.668749999999996\n",
      "Epoch: 17 Train Loss: 1.5951414043200942 Train Accuracy: 41.631249999999994\n",
      "Epoch: 18 Train Loss: 1.5941955973620376 Train Accuracy: 42.7375\n",
      "Epoch: 19 Train Loss: 1.593184761172395 Train Accuracy: 43.84375\n",
      "Epoch: 20 Train Loss: 1.592102156627458 Train Accuracy: 44.9625\n",
      "Epoch: 21 Train Loss: 1.590940219283388 Train Accuracy: 46.13125\n",
      "Epoch: 22 Train Loss: 1.5896904350338286 Train Accuracy: 47.34375\n",
      "Epoch: 23 Train Loss: 1.5883431943129105 Train Accuracy: 48.5625\n",
      "Epoch: 24 Train Loss: 1.5868876207457046 Train Accuracy: 49.84375\n",
      "Epoch: 25 Train Loss: 1.5853113693516694 Train Accuracy: 51.09375000000001\n",
      "Epoch: 26 Train Loss: 1.583600388484275 Train Accuracy: 51.9625\n",
      "Epoch: 27 Train Loss: 1.581738638648971 Train Accuracy: 52.93125\n",
      "Epoch: 28 Train Loss: 1.5797077602151712 Train Accuracy: 53.77499999999999\n",
      "Epoch: 29 Train Loss: 1.5774866809064103 Train Accuracy: 54.237500000000004\n",
      "Epoch: 30 Train Loss: 1.5750511529794893 Train Accuracy: 54.793749999999996\n",
      "Epoch: 31 Train Loss: 1.572373209492327 Train Accuracy: 55.012499999999996\n",
      "Epoch: 32 Train Loss: 1.569420529547204 Train Accuracy: 55.05625\n",
      "Epoch: 33 Train Loss: 1.566155704789321 Train Accuracy: 55.074999999999996\n",
      "Epoch: 34 Train Loss: 1.5625354052257383 Train Accuracy: 54.6625\n",
      "Epoch: 35 Train Loss: 1.5585094539506543 Train Accuracy: 54.03750000000001\n",
      "Epoch: 36 Train Loss: 1.5540198411609727 Train Accuracy: 53.337500000000006\n",
      "Epoch: 37 Train Loss: 1.548999742967002 Train Accuracy: 52.575\n",
      "Epoch: 38 Train Loss: 1.5433726665214744 Train Accuracy: 51.824999999999996\n",
      "Epoch: 39 Train Loss: 1.5370519271920422 Train Accuracy: 51.03124999999999\n",
      "Epoch: 40 Train Loss: 1.5299407812041725 Train Accuracy: 50.25625\n",
      "Epoch: 41 Train Loss: 1.521933685669594 Train Accuracy: 49.525000000000006\n",
      "Epoch: 42 Train Loss: 1.5129193155947744 Train Accuracy: 48.8875\n",
      "Epoch: 43 Train Loss: 1.5027860788256742 Train Accuracy: 48.35\n",
      "Epoch: 44 Train Loss: 1.491430832724313 Train Accuracy: 47.96875\n",
      "Epoch: 45 Train Loss: 1.47877117732793 Train Accuracy: 47.59375\n",
      "Epoch: 46 Train Loss: 1.464760941278425 Train Accuracy: 47.4\n",
      "Epoch: 47 Train Loss: 1.4494072580475441 Train Accuracy: 47.31875\n",
      "Epoch: 48 Train Loss: 1.4327861536014304 Train Accuracy: 47.375\n",
      "Epoch: 49 Train Loss: 1.4150523418534517 Train Accuracy: 47.53125\n",
      "Epoch: 50 Train Loss: 1.3964386872452719 Train Accuracy: 47.65\n",
      "Epoch: 51 Train Loss: 1.377242217380126 Train Accuracy: 48.10625\n",
      "Epoch: 52 Train Loss: 1.3577967958537727 Train Accuracy: 48.65625\n",
      "Epoch: 53 Train Loss: 1.3384367751315733 Train Accuracy: 49.0625\n",
      "Epoch: 54 Train Loss: 1.319459379617927 Train Accuracy: 49.5\n",
      "Epoch: 55 Train Loss: 1.301094356019343 Train Accuracy: 50.224999999999994\n",
      "Epoch: 56 Train Loss: 1.2834869253689418 Train Accuracy: 50.83125\n",
      "Epoch: 57 Train Loss: 1.2666955189177729 Train Accuracy: 51.475\n",
      "Epoch: 58 Train Loss: 1.2507015010350617 Train Accuracy: 52.1625\n",
      "Epoch: 59 Train Loss: 1.2354258501285844 Train Accuracy: 52.88125\n",
      "Epoch: 60 Train Loss: 1.2207478502404225 Train Accuracy: 53.77499999999999\n",
      "Epoch: 61 Train Loss: 1.2065223419746511 Train Accuracy: 54.43125\n",
      "Epoch: 62 Train Loss: 1.192593872016961 Train Accuracy: 55.05\n",
      "Epoch: 63 Train Loss: 1.1788074663386048 Train Accuracy: 55.56875\n",
      "Epoch: 64 Train Loss: 1.1650165390640779 Train Accuracy: 56.243750000000006\n",
      "Epoch: 65 Train Loss: 1.1510887388895639 Train Accuracy: 56.925000000000004\n",
      "Epoch: 66 Train Loss: 1.1369105110539415 Train Accuracy: 57.706250000000004\n",
      "Epoch: 67 Train Loss: 1.1223909676238164 Train Accuracy: 58.4\n",
      "Epoch: 68 Train Loss: 1.1074654073062156 Train Accuracy: 59.18125\n",
      "Epoch: 69 Train Loss: 1.092098558999162 Train Accuracy: 60.14375\n",
      "Epoch: 70 Train Loss: 1.0762873680735594 Train Accuracy: 60.993750000000006\n",
      "Epoch: 71 Train Loss: 1.0600629240907748 Train Accuracy: 61.90624999999999\n",
      "Epoch: 72 Train Loss: 1.0434909721638819 Train Accuracy: 62.612500000000004\n",
      "Epoch: 73 Train Loss: 1.0266704017997712 Train Accuracy: 63.349999999999994\n",
      "Epoch: 74 Train Loss: 1.0097292110638894 Train Accuracy: 64.17500000000001\n",
      "Epoch: 75 Train Loss: 0.9928177400485769 Train Accuracy: 64.8625\n",
      "Epoch: 76 Train Loss: 0.9760994295549592 Train Accuracy: 65.4625\n",
      "Epoch: 77 Train Loss: 0.959739914752742 Train Accuracy: 66.15625\n",
      "Epoch: 78 Train Loss: 0.9438957284776422 Train Accuracy: 66.65625\n",
      "Epoch: 79 Train Loss: 0.9287040881881541 Train Accuracy: 67.25625\n",
      "Epoch: 80 Train Loss: 0.9142751546970739 Train Accuracy: 67.73125\n",
      "Epoch: 81 Train Loss: 0.9006874509355112 Train Accuracy: 68.175\n",
      "Epoch: 82 Train Loss: 0.8879872197434087 Train Accuracy: 68.53125\n",
      "Epoch: 83 Train Loss: 0.8761905500982506 Train Accuracy: 68.86875\n",
      "Epoch: 84 Train Loss: 0.8652935833267342 Train Accuracy: 69.3\n",
      "Epoch: 85 Train Loss: 0.8552981405150197 Train Accuracy: 69.6125\n",
      "Epoch: 86 Train Loss: 0.8463553052274244 Train Accuracy: 70.09375\n",
      "Epoch: 87 Train Loss: 0.8394547749042661 Train Accuracy: 69.94375000000001\n",
      "Epoch: 88 Train Loss: 0.8402236449599276 Train Accuracy: 70.28750000000001\n",
      "Epoch: 89 Train Loss: 0.8792966700245257 Train Accuracy: 64.46875\n",
      "Epoch: 90 Train Loss: 1.1012904597781106 Train Accuracy: 53.625\n",
      "Epoch: 91 Train Loss: 2.01644118391901 Train Accuracy: 34.068749999999994\n",
      "Epoch: 92 Train Loss: 2.8417375268585117 Train Accuracy: 32.95\n",
      "Epoch: 93 Train Loss: 1.8353124613252247 Train Accuracy: 41.09375\n",
      "Epoch: 94 Train Loss: 1.315044162790114 Train Accuracy: 50.21875\n",
      "Epoch: 95 Train Loss: 1.4618573335805882 Train Accuracy: 49.85625\n",
      "Epoch: 96 Train Loss: 1.108108125100533 Train Accuracy: 56.63125\n",
      "Epoch: 97 Train Loss: 1.1558084910325508 Train Accuracy: 53.91875\n",
      "Epoch: 98 Train Loss: 1.0090191310933576 Train Accuracy: 62.11875\n",
      "Epoch: 99 Train Loss: 1.0164193067130998 Train Accuracy: 57.83125\n",
      "Epoch: 100 Train Loss: 0.9526772645986394 Train Accuracy: 64.60625\n",
      "Epoch: 101 Train Loss: 0.9234405082454921 Train Accuracy: 61.76875\n",
      "Epoch: 102 Train Loss: 0.8941497162521017 Train Accuracy: 67.175\n",
      "Epoch: 103 Train Loss: 0.8664725625839788 Train Accuracy: 65.64375\n",
      "Epoch: 104 Train Loss: 0.848002983792782 Train Accuracy: 69.36875\n",
      "Epoch: 105 Train Loss: 0.827968814516076 Train Accuracy: 68.1875\n",
      "Epoch: 106 Train Loss: 0.8166067371408414 Train Accuracy: 70.6625\n",
      "Epoch: 107 Train Loss: 0.8027947906730474 Train Accuracy: 69.91875\n",
      "Epoch: 108 Train Loss: 0.7953229802182928 Train Accuracy: 71.40625\n",
      "Epoch: 109 Train Loss: 0.7854450834424005 Train Accuracy: 70.89375\n",
      "Epoch: 110 Train Loss: 0.7801291728525986 Train Accuracy: 72.05625\n",
      "Epoch: 111 Train Loss: 0.7726150050765314 Train Accuracy: 71.6875\n",
      "Epoch: 112 Train Loss: 0.7686148181542415 Train Accuracy: 72.6\n",
      "Epoch: 113 Train Loss: 0.762552585061165 Train Accuracy: 72.2\n",
      "Epoch: 114 Train Loss: 0.7594989348232268 Train Accuracy: 72.96249999999999\n",
      "Epoch: 115 Train Loss: 0.754385066266053 Train Accuracy: 72.74374999999999\n",
      "Epoch: 116 Train Loss: 0.7521761303411894 Train Accuracy: 73.1875\n",
      "Epoch: 117 Train Loss: 0.747739732794667 Train Accuracy: 72.99375\n",
      "Epoch: 118 Train Loss: 0.7464602531493443 Train Accuracy: 73.34375\n",
      "Epoch: 119 Train Loss: 0.7425554292666475 Train Accuracy: 73.20625\n",
      "Epoch: 120 Train Loss: 0.7424557345593434 Train Accuracy: 73.41875\n",
      "Epoch: 121 Train Loss: 0.7389785625982139 Train Accuracy: 73.15625\n",
      "Epoch: 122 Train Loss: 0.7404712317142884 Train Accuracy: 73.43125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 123 Train Loss: 0.7372566465526292 Train Accuracy: 72.95625\n",
      "Epoch: 124 Train Loss: 0.7408791769676362 Train Accuracy: 73.2625\n",
      "Epoch: 125 Train Loss: 0.7375397214503708 Train Accuracy: 72.66875\n",
      "Epoch: 126 Train Loss: 0.7438041492603271 Train Accuracy: 72.8625\n",
      "Epoch: 127 Train Loss: 0.739527215874379 Train Accuracy: 72.15625\n",
      "Epoch: 128 Train Loss: 0.7485915667588378 Train Accuracy: 72.4375\n",
      "Epoch: 129 Train Loss: 0.7420720406599731 Train Accuracy: 71.89375\n",
      "Epoch: 130 Train Loss: 0.7533397232070315 Train Accuracy: 71.89999999999999\n",
      "Epoch: 131 Train Loss: 0.7431771236829101 Train Accuracy: 71.6375\n",
      "Epoch: 132 Train Loss: 0.7552183828095118 Train Accuracy: 71.63125000000001\n",
      "Epoch: 133 Train Loss: 0.7408036526522573 Train Accuracy: 71.72500000000001\n",
      "Epoch: 134 Train Loss: 0.7519151147035503 Train Accuracy: 71.64375\n",
      "Epoch: 135 Train Loss: 0.7340611170335667 Train Accuracy: 72.20625000000001\n",
      "Epoch: 136 Train Loss: 0.7430943861424291 Train Accuracy: 71.95625\n",
      "Epoch: 137 Train Loss: 0.7236557541093823 Train Accuracy: 72.75\n",
      "Epoch: 138 Train Loss: 0.7303892787455153 Train Accuracy: 72.39999999999999\n",
      "Epoch: 139 Train Loss: 0.7112371948042232 Train Accuracy: 73.475\n",
      "Epoch: 140 Train Loss: 0.7161377707170575 Train Accuracy: 73.01875\n",
      "Epoch: 141 Train Loss: 0.6984279780995765 Train Accuracy: 74.24374999999999\n",
      "Epoch: 142 Train Loss: 0.7022176278193409 Train Accuracy: 73.63125000000001\n",
      "Epoch: 143 Train Loss: 0.6863027481307448 Train Accuracy: 74.91250000000001\n",
      "Epoch: 144 Train Loss: 0.6896737041575337 Train Accuracy: 74.14375\n",
      "Epoch: 145 Train Loss: 0.6753776768411229 Train Accuracy: 75.44375\n",
      "Epoch: 146 Train Loss: 0.6788953208683507 Train Accuracy: 74.4375\n",
      "Epoch: 147 Train Loss: 0.6658081550640772 Train Accuracy: 75.79375\n",
      "Epoch: 148 Train Loss: 0.6699031676096205 Train Accuracy: 74.70625\n",
      "Epoch: 149 Train Loss: 0.6575683710655332 Train Accuracy: 76.14375\n",
      "Epoch: 150 Train Loss: 0.6625309017704798 Train Accuracy: 74.91250000000001\n",
      "Epoch: 151 Train Loss: 0.6505479774367557 Train Accuracy: 76.47500000000001\n",
      "Epoch: 152 Train Loss: 0.6564864839384184 Train Accuracy: 75.0375\n",
      "Epoch: 153 Train Loss: 0.6445855992746254 Train Accuracy: 76.5625\n",
      "Epoch: 154 Train Loss: 0.6513605847173494 Train Accuracy: 74.94375\n",
      "Epoch: 155 Train Loss: 0.6394797657034053 Train Accuracy: 76.71249999999999\n",
      "Epoch: 156 Train Loss: 0.6466626766820446 Train Accuracy: 74.94375\n",
      "Epoch: 157 Train Loss: 0.6350064942024543 Train Accuracy: 76.7375\n",
      "Epoch: 158 Train Loss: 0.6419232517409029 Train Accuracy: 74.95625\n",
      "Epoch: 159 Train Loss: 0.6309444053074893 Train Accuracy: 76.71249999999999\n",
      "Epoch: 160 Train Loss: 0.6368161703574308 Train Accuracy: 75.08749999999999\n",
      "Epoch: 161 Train Loss: 0.6270879029347639 Train Accuracy: 76.76875\n",
      "Epoch: 162 Train Loss: 0.6312112850426431 Train Accuracy: 75.3875\n",
      "Epoch: 163 Train Loss: 0.6232396578142663 Train Accuracy: 76.94375\n",
      "Epoch: 164 Train Loss: 0.6251280889513914 Train Accuracy: 75.925\n",
      "Epoch: 165 Train Loss: 0.6191997013833761 Train Accuracy: 77.125\n",
      "Epoch: 166 Train Loss: 0.6186499312885907 Train Accuracy: 76.56875\n",
      "Epoch: 167 Train Loss: 0.614776271451087 Train Accuracy: 77.325\n",
      "Epoch: 168 Train Loss: 0.6118698792792172 Train Accuracy: 77.08749999999999\n",
      "Epoch: 169 Train Loss: 0.6098260940679281 Train Accuracy: 77.60625\n",
      "Epoch: 170 Train Loss: 0.6048872107425172 Train Accuracy: 77.63125\n",
      "Epoch: 171 Train Loss: 0.6043058829288537 Train Accuracy: 77.7875\n",
      "Epoch: 172 Train Loss: 0.5978284688733218 Train Accuracy: 78.28125\n",
      "Epoch: 173 Train Loss: 0.5983029184192177 Train Accuracy: 78.14999999999999\n",
      "Epoch: 174 Train Loss: 0.5908595880723921 Train Accuracy: 78.85625\n",
      "Epoch: 175 Train Loss: 0.5920229263002453 Train Accuracy: 78.4\n",
      "Epoch: 176 Train Loss: 0.5841756772970296 Train Accuracy: 79.28125\n",
      "Epoch: 177 Train Loss: 0.5857414369285683 Train Accuracy: 78.7\n",
      "Epoch: 178 Train Loss: 0.5779790186298237 Train Accuracy: 79.9\n",
      "Epoch: 179 Train Loss: 0.5797472390231617 Train Accuracy: 78.88125000000001\n",
      "Epoch: 180 Train Loss: 0.5724648100953437 Train Accuracy: 80.20625\n",
      "Epoch: 181 Train Loss: 0.5743063549287883 Train Accuracy: 78.9875\n",
      "Epoch: 182 Train Loss: 0.5678258858795078 Train Accuracy: 80.5125\n",
      "Epoch: 183 Train Loss: 0.5696568960569578 Train Accuracy: 79.125\n",
      "Epoch: 184 Train Loss: 0.564272425598874 Train Accuracy: 80.7375\n",
      "Epoch: 185 Train Loss: 0.5660247619856968 Train Accuracy: 79.16875\n",
      "Epoch: 186 Train Loss: 0.5620473100642496 Train Accuracy: 80.925\n",
      "Epoch: 187 Train Loss: 0.563633972127561 Train Accuracy: 79.07499999999999\n",
      "Epoch: 188 Train Loss: 0.5613981163685877 Train Accuracy: 81.16875\n",
      "Epoch: 189 Train Loss: 0.5626684012396833 Train Accuracy: 78.9375\n",
      "Epoch: 190 Train Loss: 0.5624419169664925 Train Accuracy: 81.08125000000001\n",
      "Epoch: 191 Train Loss: 0.5631291927193254 Train Accuracy: 78.76875\n",
      "Epoch: 192 Train Loss: 0.5648732695522042 Train Accuracy: 80.9125\n",
      "Epoch: 193 Train Loss: 0.564580134078775 Train Accuracy: 78.57499999999999\n",
      "Epoch: 194 Train Loss: 0.5676394958105334 Train Accuracy: 80.7\n",
      "Epoch: 195 Train Loss: 0.565967416029293 Train Accuracy: 78.475\n",
      "Epoch: 196 Train Loss: 0.5690360637035059 Train Accuracy: 80.63749999999999\n",
      "Epoch: 197 Train Loss: 0.5658889331116518 Train Accuracy: 78.53750000000001\n",
      "Epoch: 198 Train Loss: 0.5675678698779391 Train Accuracy: 80.66875\n",
      "Epoch: 199 Train Loss: 0.5633496242671995 Train Accuracy: 78.71249999999999\n",
      "Epoch: 200 Train Loss: 0.5629220178306678 Train Accuracy: 80.81875\n",
      "Epoch: 201 Train Loss: 0.5583336300707022 Train Accuracy: 78.9875\n",
      "Epoch: 202 Train Loss: 0.5559875915322127 Train Accuracy: 81.14375\n",
      "Epoch: 203 Train Loss: 0.5516496518758089 Train Accuracy: 79.30625\n",
      "Epoch: 204 Train Loss: 0.5480730014237027 Train Accuracy: 81.375\n",
      "Epoch: 205 Train Loss: 0.5443460101529478 Train Accuracy: 79.75625\n",
      "Epoch: 206 Train Loss: 0.5402342812626055 Train Accuracy: 81.71875\n",
      "Epoch: 207 Train Loss: 0.5372505970921448 Train Accuracy: 80.2375\n",
      "Epoch: 208 Train Loss: 0.5330707135961332 Train Accuracy: 82.075\n",
      "Epoch: 209 Train Loss: 0.5308290023267083 Train Accuracy: 80.65625\n",
      "Epoch: 210 Train Loss: 0.5268093854754192 Train Accuracy: 82.21875\n",
      "Epoch: 211 Train Loss: 0.525251335804496 Train Accuracy: 80.84375\n",
      "Epoch: 212 Train Loss: 0.5214583095420386 Train Accuracy: 82.44375000000001\n",
      "Epoch: 213 Train Loss: 0.5205143339015779 Train Accuracy: 80.98125\n",
      "Epoch: 214 Train Loss: 0.5169268802916706 Train Accuracy: 82.5875\n",
      "Epoch: 215 Train Loss: 0.5165382860907825 Train Accuracy: 81.26875\n",
      "Epoch: 216 Train Loss: 0.5130958530899737 Train Accuracy: 82.69375\n",
      "Epoch: 217 Train Loss: 0.5132227461993735 Train Accuracy: 81.43125\n",
      "Epoch: 218 Train Loss: 0.5098496499629303 Train Accuracy: 82.75\n",
      "Epoch: 219 Train Loss: 0.5104713006599908 Train Accuracy: 81.44375000000001\n",
      "Epoch: 220 Train Loss: 0.5070871417516502 Train Accuracy: 82.83125\n",
      "Epoch: 221 Train Loss: 0.5081987763591516 Train Accuracy: 81.58125000000001\n",
      "Epoch: 222 Train Loss: 0.5047222627897666 Train Accuracy: 82.90625\n",
      "Epoch: 223 Train Loss: 0.5063302491077072 Train Accuracy: 81.69999999999999\n",
      "Epoch: 224 Train Loss: 0.5026809025852328 Train Accuracy: 82.95\n",
      "Epoch: 225 Train Loss: 0.5047971619319754 Train Accuracy: 81.8125\n",
      "Epoch: 226 Train Loss: 0.5008973244184379 Train Accuracy: 83.0\n",
      "Epoch: 227 Train Loss: 0.5035333127582476 Train Accuracy: 81.81875\n",
      "Epoch: 228 Train Loss: 0.499311572878971 Train Accuracy: 83.00625000000001\n",
      "Epoch: 229 Train Loss: 0.5024720806214935 Train Accuracy: 81.825\n",
      "Epoch: 230 Train Loss: 0.4978683161040865 Train Accuracy: 82.96249999999999\n",
      "Epoch: 231 Train Loss: 0.5015454479398425 Train Accuracy: 81.875\n",
      "Epoch: 232 Train Loss: 0.49651691539744297 Train Accuracy: 82.95\n",
      "Epoch: 233 Train Loss: 0.5006847936500639 Train Accuracy: 81.89999999999999\n",
      "Epoch: 234 Train Loss: 0.4952121069531814 Train Accuracy: 82.91875\n",
      "Epoch: 235 Train Loss: 0.49982300276509606 Train Accuracy: 81.88125000000001\n",
      "Epoch: 236 Train Loss: 0.493914578574138 Train Accuracy: 82.96875\n",
      "Epoch: 237 Train Loss: 0.4988972474969778 Train Accuracy: 81.9375\n",
      "Epoch: 238 Train Loss: 0.49259098435964066 Train Accuracy: 82.96249999999999\n",
      "Epoch: 239 Train Loss: 0.4978519032415984 Train Accuracy: 81.99375\n",
      "Epoch: 240 Train Loss: 0.49121345284058354 Train Accuracy: 82.975\n",
      "Epoch: 241 Train Loss: 0.49664135560210443 Train Accuracy: 82.04375\n",
      "Epoch: 242 Train Loss: 0.4897591132280012 Train Accuracy: 82.9375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 243 Train Loss: 0.49523266329079685 Train Accuracy: 82.05\n",
      "Epoch: 244 Train Loss: 0.48821026616594987 Train Accuracy: 82.92500000000001\n",
      "Epoch: 245 Train Loss: 0.49360794459896884 Train Accuracy: 82.15\n",
      "Epoch: 246 Train Loss: 0.4865554426091889 Train Accuracy: 82.93125\n",
      "Epoch: 247 Train Loss: 0.491766001570929 Train Accuracy: 82.30625\n",
      "Epoch: 248 Train Loss: 0.4847909447060113 Train Accuracy: 82.975\n",
      "Epoch: 249 Train Loss: 0.4897224216086065 Train Accuracy: 82.4125\n",
      "Epoch: 250 Train Loss: 0.4829219840165378 Train Accuracy: 83.025\n",
      "Epoch: 251 Train Loss: 0.4875075578627635 Train Accuracy: 82.48124999999999\n",
      "Epoch: 252 Train Loss: 0.4809625729081271 Train Accuracy: 83.1375\n",
      "Epoch: 253 Train Loss: 0.48516242933208076 Train Accuracy: 82.5875\n",
      "Epoch: 254 Train Loss: 0.4789338695484546 Train Accuracy: 83.18124999999999\n",
      "Epoch: 255 Train Loss: 0.48273334648756605 Train Accuracy: 82.72500000000001\n",
      "Epoch: 256 Train Loss: 0.47686136173811955 Train Accuracy: 83.19375\n",
      "Epoch: 257 Train Loss: 0.4802664910790807 Train Accuracy: 82.9125\n",
      "Epoch: 258 Train Loss: 0.47477168605312825 Train Accuracy: 83.3\n",
      "Epoch: 259 Train Loss: 0.4778035490672849 Train Accuracy: 82.9875\n",
      "Epoch: 260 Train Loss: 0.4726898544834202 Train Accuracy: 83.35625\n",
      "Epoch: 261 Train Loss: 0.4753789740236172 Train Accuracy: 83.04375\n",
      "Epoch: 262 Train Loss: 0.4706373367246042 Train Accuracy: 83.4125\n",
      "Epoch: 263 Train Loss: 0.4730188821060195 Train Accuracy: 83.175\n",
      "Epoch: 264 Train Loss: 0.46863107320757735 Train Accuracy: 83.46249999999999\n",
      "Epoch: 265 Train Loss: 0.47074120157488475 Train Accuracy: 83.25625\n",
      "Epoch: 266 Train Loss: 0.4666832449710311 Train Accuracy: 83.48124999999999\n",
      "Epoch: 267 Train Loss: 0.46855658055336297 Train Accuracy: 83.35625\n",
      "Epoch: 268 Train Loss: 0.46480153836219734 Train Accuracy: 83.54375\n",
      "Epoch: 269 Train Loss: 0.4664696208432498 Train Accuracy: 83.45625\n",
      "Epoch: 270 Train Loss: 0.4629896661855923 Train Accuracy: 83.65625\n",
      "Epoch: 271 Train Loss: 0.4644801446891986 Train Accuracy: 83.54375\n",
      "Epoch: 272 Train Loss: 0.4612479770070589 Train Accuracy: 83.7125\n",
      "Epoch: 273 Train Loss: 0.46258433821215994 Train Accuracy: 83.5625\n",
      "Epoch: 274 Train Loss: 0.45957405642503135 Train Accuracy: 83.775\n",
      "Epoch: 275 Train Loss: 0.46077571606511686 Train Accuracy: 83.60625\n",
      "Epoch: 276 Train Loss: 0.45796327911885953 Train Accuracy: 83.81875\n",
      "Epoch: 277 Train Loss: 0.4590459124796921 Train Accuracy: 83.73125\n",
      "Epoch: 278 Train Loss: 0.45640930524470774 Train Accuracy: 83.86875\n",
      "Epoch: 279 Train Loss: 0.45738533269163795 Train Accuracy: 83.75\n",
      "Epoch: 280 Train Loss: 0.45490453268307346 Train Accuracy: 83.89375\n",
      "Epoch: 281 Train Loss: 0.4557837063352823 Train Accuracy: 83.8125\n",
      "Epoch: 282 Train Loss: 0.4534405224011219 Train Accuracy: 83.95\n",
      "Epoch: 283 Train Loss: 0.45423057942058054 Train Accuracy: 83.86875\n",
      "Epoch: 284 Train Loss: 0.45200841184129514 Train Accuracy: 83.9875\n",
      "Epoch: 285 Train Loss: 0.45271577015036035 Train Accuracy: 83.9125\n",
      "Epoch: 286 Train Loss: 0.45059932415501286 Train Accuracy: 84.03125\n",
      "Epoch: 287 Train Loss: 0.451229800617554 Train Accuracy: 83.98125\n",
      "Epoch: 288 Train Loss: 0.44920477231648304 Train Accuracy: 84.0625\n",
      "Epoch: 289 Train Loss: 0.44976430495057745 Train Accuracy: 84.0125\n",
      "Epoch: 290 Train Loss: 0.44781704964904734 Train Accuracy: 84.11874999999999\n",
      "Epoch: 291 Train Loss: 0.44831240778890885 Train Accuracy: 84.075\n",
      "Epoch: 292 Train Loss: 0.4464295949048558 Train Accuracy: 84.1625\n",
      "Epoch: 293 Train Loss: 0.44686906746646315 Train Accuracy: 84.11874999999999\n",
      "Epoch: 294 Train Loss: 0.4450373231472639 Train Accuracy: 84.21875\n",
      "Epoch: 295 Train Loss: 0.44543138745669003 Train Accuracy: 84.1625\n",
      "Epoch: 296 Train Loss: 0.44363692492613516 Train Accuracy: 84.25625\n",
      "Epoch: 297 Train Loss: 0.44399891795060453 Train Accuracy: 84.2\n",
      "Epoch: 298 Train Loss: 0.4422271563861961 Train Accuracy: 84.3\n",
      "Epoch: 299 Train Loss: 0.4425739964655106 Train Accuracy: 84.28750000000001\n",
      "Epoch: 300 Train Loss: 0.4408091721728736 Train Accuracy: 84.34375\n",
      "Epoch: 301 Train Loss: 0.4411622111246294 Train Accuracy: 84.3375\n",
      "Epoch: 302 Train Loss: 0.4393869912898989 Train Accuracy: 84.4125\n",
      "Epoch: 303 Train Loss: 0.43977311111779793 Train Accuracy: 84.34375\n",
      "Epoch: 304 Train Loss: 0.4379682332919376 Train Accuracy: 84.5\n",
      "Epoch: 305 Train Loss: 0.438421332073086 Train Accuracy: 84.35625\n",
      "Epoch: 306 Train Loss: 0.4365653165145753 Train Accuracy: 84.61874999999999\n",
      "Epoch: 307 Train Loss: 0.4371283382404691 Train Accuracy: 84.39375\n",
      "Epoch: 308 Train Loss: 0.43519736203148773 Train Accuracy: 84.65\n",
      "Epoch: 309 Train Loss: 0.43592497604668096 Train Accuracy: 84.425\n",
      "Epoch: 310 Train Loss: 0.4338930645268766 Train Accuracy: 84.75\n",
      "Epoch: 311 Train Loss: 0.4348549028082363 Train Accuracy: 84.46249999999999\n",
      "Epoch: 312 Train Loss: 0.4326946860182554 Train Accuracy: 84.82499999999999\n",
      "Epoch: 313 Train Loss: 0.4339785178050523 Train Accuracy: 84.46249999999999\n",
      "Epoch: 314 Train Loss: 0.43166289380737716 Train Accuracy: 84.85000000000001\n",
      "Epoch: 315 Train Loss: 0.4333759312918938 Train Accuracy: 84.45\n",
      "Epoch: 316 Train Loss: 0.4308809920145794 Train Accuracy: 84.95\n",
      "Epoch: 317 Train Loss: 0.4331452444081026 Train Accuracy: 84.40625\n",
      "Epoch: 318 Train Loss: 0.4304545764508092 Train Accuracy: 85.05\n",
      "Epoch: 319 Train Loss: 0.4333886725972884 Train Accuracy: 84.36875\n",
      "Epoch: 320 Train Loss: 0.43049844128512493 Train Accuracy: 85.14375\n",
      "Epoch: 321 Train Loss: 0.43417520547043287 Train Accuracy: 84.41875\n",
      "Epoch: 322 Train Loss: 0.43109845200584535 Train Accuracy: 85.1875\n",
      "Epoch: 323 Train Loss: 0.43547080979211894 Train Accuracy: 84.21875\n",
      "Epoch: 324 Train Loss: 0.43223953034812734 Train Accuracy: 85.175\n",
      "Epoch: 325 Train Loss: 0.4370486428141429 Train Accuracy: 84.05625\n",
      "Epoch: 326 Train Loss: 0.4337156111280459 Train Accuracy: 85.2125\n",
      "Epoch: 327 Train Loss: 0.43843940896010064 Train Accuracy: 83.9375\n",
      "Epoch: 328 Train Loss: 0.43508584584651416 Train Accuracy: 85.21875\n",
      "Epoch: 329 Train Loss: 0.43901826473448824 Train Accuracy: 83.90625\n",
      "Epoch: 330 Train Loss: 0.43576640943555733 Train Accuracy: 85.18124999999999\n",
      "Epoch: 331 Train Loss: 0.4382581177522362 Train Accuracy: 83.94375\n",
      "Epoch: 332 Train Loss: 0.4352679163878019 Train Accuracy: 85.25\n",
      "Epoch: 333 Train Loss: 0.43600201640526076 Train Accuracy: 83.98125\n",
      "Epoch: 334 Train Loss: 0.43343286703066186 Train Accuracy: 85.31875\n",
      "Epoch: 335 Train Loss: 0.4325309141251476 Train Accuracy: 84.075\n",
      "Epoch: 336 Train Loss: 0.43048891203967704 Train Accuracy: 85.32499999999999\n",
      "Epoch: 337 Train Loss: 0.4283837704111408 Train Accuracy: 84.3375\n",
      "Epoch: 338 Train Loss: 0.42689572077364996 Train Accuracy: 85.3625\n",
      "Epoch: 339 Train Loss: 0.4241033378661474 Train Accuracy: 84.59375\n",
      "Epoch: 340 Train Loss: 0.4231294582792072 Train Accuracy: 85.46249999999999\n",
      "Epoch: 341 Train Loss: 0.4200783284734904 Train Accuracy: 84.775\n",
      "Epoch: 342 Train Loss: 0.4195437341907993 Train Accuracy: 85.575\n",
      "Epoch: 343 Train Loss: 0.41651462807730594 Train Accuracy: 84.96249999999999\n",
      "Epoch: 344 Train Loss: 0.41633785432108367 Train Accuracy: 85.7\n",
      "Epoch: 345 Train Loss: 0.4134798138063354 Train Accuracy: 85.01875\n",
      "Epoch: 346 Train Loss: 0.41358962285168577 Train Accuracy: 85.775\n",
      "Epoch: 347 Train Loss: 0.4109624851631114 Train Accuracy: 85.06875000000001\n",
      "Epoch: 348 Train Loss: 0.41130389119526434 Train Accuracy: 85.81875\n",
      "Epoch: 349 Train Loss: 0.40891766539664 Train Accuracy: 85.15\n",
      "Epoch: 350 Train Loss: 0.4094511200962542 Train Accuracy: 85.88125\n",
      "Epoch: 351 Train Loss: 0.4072928472941539 Train Accuracy: 85.2125\n",
      "Epoch: 352 Train Loss: 0.4079899266565732 Train Accuracy: 85.91250000000001\n",
      "Epoch: 353 Train Loss: 0.40603879769650814 Train Accuracy: 85.19375\n",
      "Epoch: 354 Train Loss: 0.4068762971017943 Train Accuracy: 85.91875\n",
      "Epoch: 355 Train Loss: 0.4051105265783545 Train Accuracy: 85.2\n",
      "Epoch: 356 Train Loss: 0.40606385714978266 Train Accuracy: 85.91875\n",
      "Epoch: 357 Train Loss: 0.40446249145068014 Train Accuracy: 85.24374999999999\n",
      "Epoch: 358 Train Loss: 0.4054989959612235 Train Accuracy: 85.91250000000001\n",
      "Epoch: 359 Train Loss: 0.40404098019557116 Train Accuracy: 85.20625\n",
      "Epoch: 360 Train Loss: 0.40511419620295 Train Accuracy: 85.875\n",
      "Epoch: 361 Train Loss: 0.4037765304639273 Train Accuracy: 85.1875\n",
      "Epoch: 362 Train Loss: 0.4048232205524799 Train Accuracy: 85.875\n",
      "Epoch: 363 Train Loss: 0.4035798825797701 Train Accuracy: 85.2375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 364 Train Loss: 0.4045221565639647 Train Accuracy: 85.90625\n",
      "Epoch: 365 Train Loss: 0.40334516845803203 Train Accuracy: 85.29375\n",
      "Epoch: 366 Train Loss: 0.40409926815435215 Train Accuracy: 85.9375\n",
      "Epoch: 367 Train Loss: 0.4029623163755608 Train Accuracy: 85.3125\n",
      "Epoch: 368 Train Loss: 0.4034530604400039 Train Accuracy: 85.91875\n",
      "Epoch: 369 Train Loss: 0.40233650863946424 Train Accuracy: 85.28125\n",
      "Epoch: 370 Train Loss: 0.4025129656068881 Train Accuracy: 85.925\n",
      "Epoch: 371 Train Loss: 0.40140790069760723 Train Accuracy: 85.29375\n",
      "Epoch: 372 Train Loss: 0.40125410131261247 Train Accuracy: 85.89375\n",
      "Epoch: 373 Train Loss: 0.4001633450191634 Train Accuracy: 85.34375\n",
      "Epoch: 374 Train Loss: 0.39969974209503983 Train Accuracy: 85.975\n",
      "Epoch: 375 Train Loss: 0.3986354849636367 Train Accuracy: 85.41250000000001\n",
      "Epoch: 376 Train Loss: 0.3979114574780672 Train Accuracy: 86.01875\n",
      "Epoch: 377 Train Loss: 0.396890949610984 Train Accuracy: 85.48125\n",
      "Epoch: 378 Train Loss: 0.3959726373685848 Train Accuracy: 86.0625\n",
      "Epoch: 379 Train Loss: 0.3950139408198625 Train Accuracy: 85.59375\n",
      "Epoch: 380 Train Loss: 0.39397269839218635 Train Accuracy: 86.13125000000001\n",
      "Epoch: 381 Train Loss: 0.39309187861880446 Train Accuracy: 85.63125000000001\n",
      "Epoch: 382 Train Loss: 0.3919970239501804 Train Accuracy: 86.175\n",
      "Epoch: 383 Train Loss: 0.39120718235206536 Train Accuracy: 85.7125\n",
      "Epoch: 384 Train Loss: 0.39012443977542527 Train Accuracy: 86.1625\n",
      "Epoch: 385 Train Loss: 0.38943633985776976 Train Accuracy: 85.78750000000001\n",
      "Epoch: 386 Train Loss: 0.38843198390376854 Train Accuracy: 86.15\n",
      "Epoch: 387 Train Loss: 0.38785581660282825 Train Accuracy: 85.86875\n",
      "Epoch: 388 Train Loss: 0.38700642334132473 Train Accuracy: 86.10624999999999\n",
      "Epoch: 389 Train Loss: 0.3865542578375095 Train Accuracy: 86.1125\n",
      "Epoch: 390 Train Loss: 0.38596280366501173 Train Accuracy: 86.2\n",
      "Epoch: 391 Train Loss: 0.38565107743170823 Train Accuracy: 86.25625\n",
      "Epoch: 392 Train Loss: 0.3854711064360298 Train Accuracy: 86.1375\n",
      "Epoch: 393 Train Loss: 0.3853214005807015 Train Accuracy: 86.41875\n",
      "Epoch: 394 Train Loss: 0.3857904458162292 Train Accuracy: 86.14375\n",
      "Epoch: 395 Train Loss: 0.3858231773192732 Train Accuracy: 86.47500000000001\n",
      "Epoch: 396 Train Loss: 0.3873002379984639 Train Accuracy: 86.04375\n",
      "Epoch: 397 Train Loss: 0.3875067445166479 Train Accuracy: 86.41875\n",
      "Epoch: 398 Train Loss: 0.39048666788790637 Train Accuracy: 85.90625\n",
      "Epoch: 399 Train Loss: 0.39074855050482576 Train Accuracy: 86.2875\n",
      "Epoch: 400 Train Loss: 0.39577716683138703 Train Accuracy: 85.53125\n",
      "Epoch: 401 Train Loss: 0.3956953405053954 Train Accuracy: 86.08125\n",
      "Epoch: 402 Train Loss: 0.4030594149544911 Train Accuracy: 85.3125\n",
      "Epoch: 403 Train Loss: 0.4017448953836532 Train Accuracy: 85.8625\n",
      "Epoch: 404 Train Loss: 0.4109240589807601 Train Accuracy: 84.98125\n",
      "Epoch: 405 Train Loss: 0.40710121825778595 Train Accuracy: 85.675\n",
      "Epoch: 406 Train Loss: 0.4164586169714274 Train Accuracy: 84.68124999999999\n",
      "Epoch: 407 Train Loss: 0.40935250228695175 Train Accuracy: 85.61875\n",
      "Epoch: 408 Train Loss: 0.41685056747993476 Train Accuracy: 84.65625\n",
      "Epoch: 409 Train Loss: 0.40724666969742845 Train Accuracy: 85.73125\n",
      "Epoch: 410 Train Loss: 0.4118204152548625 Train Accuracy: 84.8375\n",
      "Epoch: 411 Train Loss: 0.40169219684337437 Train Accuracy: 85.80624999999999\n",
      "Epoch: 412 Train Loss: 0.4036836969786628 Train Accuracy: 85.3\n",
      "Epoch: 413 Train Loss: 0.39472010216039244 Train Accuracy: 86.0\n",
      "Epoch: 414 Train Loss: 0.39511696732412227 Train Accuracy: 85.55625\n",
      "Epoch: 415 Train Loss: 0.3879876791004184 Train Accuracy: 86.2625\n",
      "Epoch: 416 Train Loss: 0.3876488650159118 Train Accuracy: 85.8125\n",
      "Epoch: 417 Train Loss: 0.3822700620855636 Train Accuracy: 86.5\n",
      "Epoch: 418 Train Loss: 0.38169754819236174 Train Accuracy: 86.2\n",
      "Epoch: 419 Train Loss: 0.3777017921638492 Train Accuracy: 86.76875\n",
      "Epoch: 420 Train Loss: 0.37712456512158804 Train Accuracy: 86.38749999999999\n",
      "Epoch: 421 Train Loss: 0.37413327953393116 Train Accuracy: 86.91875\n",
      "Epoch: 422 Train Loss: 0.3736348001835746 Train Accuracy: 86.49375\n",
      "Epoch: 423 Train Loss: 0.37134919311625986 Train Accuracy: 87.02499999999999\n",
      "Epoch: 424 Train Loss: 0.37095042945685186 Train Accuracy: 86.625\n",
      "Epoch: 425 Train Loss: 0.36915807482300633 Train Accuracy: 87.2\n",
      "Epoch: 426 Train Loss: 0.3688584303580193 Train Accuracy: 86.6875\n",
      "Epoch: 427 Train Loss: 0.36741483527564167 Train Accuracy: 87.26875\n",
      "Epoch: 428 Train Loss: 0.3672093311834748 Train Accuracy: 86.7375\n",
      "Epoch: 429 Train Loss: 0.36601776376333994 Train Accuracy: 87.3375\n",
      "Epoch: 430 Train Loss: 0.365903314708792 Train Accuracy: 86.76875\n",
      "Epoch: 431 Train Loss: 0.3648990919214837 Train Accuracy: 87.4\n",
      "Epoch: 432 Train Loss: 0.3648763453252626 Train Accuracy: 86.81875\n",
      "Epoch: 433 Train Loss: 0.3640158349983063 Train Accuracy: 87.43125\n",
      "Epoch: 434 Train Loss: 0.3640893532660935 Train Accuracy: 86.8375\n",
      "Epoch: 435 Train Loss: 0.3633424787156462 Train Accuracy: 87.46875\n",
      "Epoch: 436 Train Loss: 0.3635202940777864 Train Accuracy: 86.88749999999999\n",
      "Epoch: 437 Train Loss: 0.3628652882968606 Train Accuracy: 87.46875\n",
      "Epoch: 438 Train Loss: 0.3631580920450118 Train Accuracy: 86.925\n",
      "Epoch: 439 Train Loss: 0.3625775322598425 Train Accuracy: 87.45\n",
      "Epoch: 440 Train Loss: 0.36299742901940013 Train Accuracy: 86.93125\n",
      "Epoch: 441 Train Loss: 0.36247488413088713 Train Accuracy: 87.43125\n",
      "Epoch: 442 Train Loss: 0.36303351245770027 Train Accuracy: 86.89375\n",
      "Epoch: 443 Train Loss: 0.36255042569005647 Train Accuracy: 87.4375\n",
      "Epoch: 444 Train Loss: 0.3632562567597391 Train Accuracy: 86.8375\n",
      "Epoch: 445 Train Loss: 0.36278901832417004 Train Accuracy: 87.45625\n",
      "Epoch: 446 Train Loss: 0.36364382169505927 Train Accuracy: 86.83125000000001\n",
      "Epoch: 447 Train Loss: 0.36316141172612165 Train Accuracy: 87.38125\n",
      "Epoch: 448 Train Loss: 0.36415626846926397 Train Accuracy: 86.79375\n",
      "Epoch: 449 Train Loss: 0.36361933281809056 Train Accuracy: 87.31875\n",
      "Epoch: 450 Train Loss: 0.36473115492728375 Train Accuracy: 86.73125\n",
      "Epoch: 451 Train Loss: 0.3640937034566025 Train Accuracy: 87.28125\n",
      "Epoch: 452 Train Loss: 0.36528376681692964 Train Accuracy: 86.7375\n",
      "Epoch: 453 Train Loss: 0.36449845488719845 Train Accuracy: 87.2625\n",
      "Epoch: 454 Train Loss: 0.3657145245554244 Train Accuracy: 86.725\n",
      "Epoch: 455 Train Loss: 0.3647413289037038 Train Accuracy: 87.25\n",
      "Epoch: 456 Train Loss: 0.36592408807311144 Train Accuracy: 86.675\n",
      "Epoch: 457 Train Loss: 0.36474028562456823 Train Accuracy: 87.28125\n",
      "Epoch: 458 Train Loss: 0.3658329914781298 Train Accuracy: 86.675\n",
      "Epoch: 459 Train Loss: 0.364440750192952 Train Accuracy: 87.2625\n",
      "Epoch: 460 Train Loss: 0.36539920081242255 Train Accuracy: 86.675\n",
      "Epoch: 461 Train Loss: 0.3638272437554629 Train Accuracy: 87.275\n",
      "Epoch: 462 Train Loss: 0.3646265738235788 Train Accuracy: 86.7125\n",
      "Epoch: 463 Train Loss: 0.36292475656900103 Train Accuracy: 87.29375\n",
      "Epoch: 464 Train Loss: 0.3635609169669591 Train Accuracy: 86.76875\n",
      "Epoch: 465 Train Loss: 0.36178981482225964 Train Accuracy: 87.375\n",
      "Epoch: 466 Train Loss: 0.3622760217465321 Train Accuracy: 86.75\n",
      "Epoch: 467 Train Loss: 0.3604955721156125 Train Accuracy: 87.425\n",
      "Epoch: 468 Train Loss: 0.36085593140536015 Train Accuracy: 86.8375\n",
      "Epoch: 469 Train Loss: 0.3591167580874513 Train Accuracy: 87.47500000000001\n",
      "Epoch: 470 Train Loss: 0.3593796851670091 Train Accuracy: 86.9\n",
      "Epoch: 471 Train Loss: 0.3577187355003546 Train Accuracy: 87.53125\n",
      "Epoch: 472 Train Loss: 0.35791193992881465 Train Accuracy: 86.97500000000001\n",
      "Epoch: 473 Train Loss: 0.35635205532326686 Train Accuracy: 87.5625\n",
      "Epoch: 474 Train Loss: 0.3564996447504732 Train Accuracy: 87.0375\n",
      "Epoch: 475 Train Loss: 0.35505163454967764 Train Accuracy: 87.60625\n",
      "Epoch: 476 Train Loss: 0.3551730258971009 Train Accuracy: 87.14375\n",
      "Epoch: 477 Train Loss: 0.3538387476307411 Train Accuracy: 87.61875\n",
      "Epoch: 478 Train Loss: 0.3539487404958416 Train Accuracy: 87.2125\n",
      "Epoch: 479 Train Loss: 0.352724139919267 Train Accuracy: 87.6625\n",
      "Epoch: 480 Train Loss: 0.3528335491527356 Train Accuracy: 87.3125\n",
      "Epoch: 481 Train Loss: 0.35171113912931834 Train Accuracy: 87.7125\n",
      "Epoch: 482 Train Loss: 0.35182757382460783 Train Accuracy: 87.38749999999999\n",
      "Epoch: 483 Train Loss: 0.35079821411626433 Train Accuracy: 87.7875\n",
      "Epoch: 484 Train Loss: 0.3509267807847762 Train Accuracy: 87.40625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 485 Train Loss: 0.3499808267191822 Train Accuracy: 87.8125\n",
      "Epoch: 486 Train Loss: 0.35012466907685674 Train Accuracy: 87.44375\n",
      "Epoch: 487 Train Loss: 0.34925263308253884 Train Accuracy: 87.8375\n",
      "Epoch: 488 Train Loss: 0.34941329793422105 Train Accuracy: 87.47500000000001\n",
      "Epoch: 489 Train Loss: 0.34860617270986805 Train Accuracy: 87.84375\n",
      "Epoch: 490 Train Loss: 0.3487838282792486 Train Accuracy: 87.52499999999999\n",
      "Epoch: 491 Train Loss: 0.348033196286452 Train Accuracy: 87.85625\n",
      "Epoch: 492 Train Loss: 0.3482267437722751 Train Accuracy: 87.54375\n",
      "Epoch: 493 Train Loss: 0.3475247680559673 Train Accuracy: 87.8875\n",
      "Epoch: 494 Train Loss: 0.3477318912407473 Train Accuracy: 87.54375\n",
      "Epoch: 495 Train Loss: 0.3470712569067838 Train Accuracy: 87.875\n",
      "Epoch: 496 Train Loss: 0.3472884544536595 Train Accuracy: 87.58749999999999\n",
      "Epoch: 497 Train Loss: 0.34666231042581297 Train Accuracy: 87.90625\n",
      "Epoch: 498 Train Loss: 0.34688495295996125 Train Accuracy: 87.575\n",
      "Epoch: 499 Train Loss: 0.34628688833869675 Train Accuracy: 87.9\n",
      "Epoch: 500 Train Loss: 0.3465093369067788 Train Accuracy: 87.575\n",
      "Epoch: 501 Train Loss: 0.3459334127553433 Train Accuracy: 87.89375\n",
      "Epoch: 502 Train Loss: 0.34614922523654273 Train Accuracy: 87.60625\n",
      "Epoch: 503 Train Loss: 0.3455900687672178 Train Accuracy: 87.90625\n",
      "Epoch: 504 Train Loss: 0.3457923052065666 Train Accuracy: 87.6\n",
      "Epoch: 505 Train Loss: 0.3452452583821719 Train Accuracy: 87.91250000000001\n",
      "Epoch: 506 Train Loss: 0.3454268756803017 Train Accuracy: 87.6\n",
      "Epoch: 507 Train Loss: 0.34488817534897165 Train Accuracy: 87.9375\n",
      "Epoch: 508 Train Loss: 0.34504247933822385 Train Accuracy: 87.63125000000001\n",
      "Epoch: 509 Train Loss: 0.34450943410271284 Train Accuracy: 87.925\n",
      "Epoch: 510 Train Loss: 0.34463053796633625 Train Accuracy: 87.6375\n",
      "Epoch: 511 Train Loss: 0.3441016617156286 Train Accuracy: 87.925\n",
      "Epoch: 512 Train Loss: 0.3441848898010613 Train Accuracy: 87.66874999999999\n",
      "Epoch: 513 Train Loss: 0.3436599559315289 Train Accuracy: 87.9375\n",
      "Epoch: 514 Train Loss: 0.34370213542287026 Train Accuracy: 87.69375000000001\n",
      "Epoch: 515 Train Loss: 0.3431821293916508 Train Accuracy: 87.94999999999999\n",
      "Epoch: 516 Train Loss: 0.3431817293953653 Train Accuracy: 87.725\n",
      "Epoch: 517 Train Loss: 0.34266869723979265 Train Accuracy: 87.95625\n",
      "Epoch: 518 Train Loss: 0.3426258014610201 Train Accuracy: 87.75\n",
      "Epoch: 519 Train Loss: 0.3421226127025653 Train Accuracy: 87.96875\n",
      "Epoch: 520 Train Loss: 0.34203874071835755 Train Accuracy: 87.7875\n",
      "Epoch: 521 Train Loss: 0.3415487997306473 Train Accuracy: 87.9875\n",
      "Epoch: 522 Train Loss: 0.34142661516521206 Train Accuracy: 87.85\n",
      "Epoch: 523 Train Loss: 0.34095356168194807 Train Accuracy: 88.03125\n",
      "Epoch: 524 Train Loss: 0.34079651807257144 Train Accuracy: 87.85625\n",
      "Epoch: 525 Train Loss: 0.34034395439657716 Train Accuracy: 88.05625\n",
      "Epoch: 526 Train Loss: 0.3401559299223036 Train Accuracy: 87.9\n",
      "Epoch: 527 Train Loss: 0.33972720199042516 Train Accuracy: 88.08749999999999\n",
      "Epoch: 528 Train Loss: 0.33951216493838077 Train Accuracy: 87.9375\n",
      "Epoch: 529 Train Loss: 0.33911021075229447 Train Accuracy: 88.10625\n",
      "Epoch: 530 Train Loss: 0.33887194309940216 Train Accuracy: 87.98125\n",
      "Epoch: 531 Train Loss: 0.33849920910639336 Train Accuracy: 88.125\n",
      "Epoch: 532 Train Loss: 0.3382411004818717 Train Accuracy: 87.97500000000001\n",
      "Epoch: 533 Train Loss: 0.33789951692751474 Train Accuracy: 88.14375\n",
      "Epoch: 534 Train Loss: 0.337624428794242 Train Accuracy: 88.0125\n",
      "Epoch: 535 Train Loss: 0.3373154298357846 Train Accuracy: 88.19375000000001\n",
      "Epoch: 536 Train Loss: 0.3370256215190531 Train Accuracy: 88.00625\n",
      "Epoch: 537 Train Loss: 0.3367501945889567 Train Accuracy: 88.225\n",
      "Epoch: 538 Train Loss: 0.3364472988084034 Train Accuracy: 88.00625\n",
      "Epoch: 539 Train Loss: 0.33620604921944736 Train Accuracy: 88.25\n",
      "Epoch: 540 Train Loss: 0.335891084148712 Train Accuracy: 88.03125\n",
      "Epoch: 541 Train Loss: 0.33568430394951065 Train Accuracy: 88.275\n",
      "Epoch: 542 Train Loss: 0.33535771032504014 Train Accuracy: 88.03750000000001\n",
      "Epoch: 543 Train Loss: 0.33518544387950866 Train Accuracy: 88.30624999999999\n",
      "Epoch: 544 Train Loss: 0.33484713818295114 Train Accuracy: 88.05625\n",
      "Epoch: 545 Train Loss: 0.3347092401511694 Train Accuracy: 88.30624999999999\n",
      "Epoch: 546 Train Loss: 0.3343586775617384 Train Accuracy: 88.05625\n",
      "Epoch: 547 Train Loss: 0.3342548615384685 Train Accuracy: 88.35\n",
      "Epoch: 548 Train Loss: 0.3338911046724987 Train Accuracy: 88.075\n",
      "Epoch: 549 Train Loss: 0.3338209825818523 Train Accuracy: 88.375\n",
      "Epoch: 550 Train Loss: 0.3334427737514233 Train Accuracy: 88.08749999999999\n",
      "Epoch: 551 Train Loss: 0.3334058872412105 Train Accuracy: 88.41250000000001\n",
      "Epoch: 552 Train Loss: 0.3330117230049103 Train Accuracy: 88.11250000000001\n",
      "Epoch: 553 Train Loss: 0.33300756864157927 Train Accuracy: 88.40625\n",
      "Epoch: 554 Train Loss: 0.3325957758399544 Train Accuracy: 88.11875\n",
      "Epoch: 555 Train Loss: 0.3326238260043376 Train Accuracy: 88.39375000000001\n",
      "Epoch: 556 Train Loss: 0.3321926383924359 Train Accuracy: 88.14375\n",
      "Epoch: 557 Train Loss: 0.33225235954343046 Train Accuracy: 88.40625\n",
      "Epoch: 558 Train Loss: 0.3317999937146607 Train Accuracy: 88.175\n",
      "Epoch: 559 Train Loss: 0.331890863237263 Train Accuracy: 88.40625\n",
      "Epoch: 560 Train Loss: 0.33141559195732423 Train Accuracy: 88.2\n",
      "Epoch: 561 Train Loss: 0.33153711424760746 Train Accuracy: 88.41875\n",
      "Epoch: 562 Train Loss: 0.3310373347669035 Train Accuracy: 88.21875\n",
      "Epoch: 563 Train Loss: 0.3311890566249641 Train Accuracy: 88.43124999999999\n",
      "Epoch: 564 Train Loss: 0.33066335117757656 Train Accuracy: 88.24375\n",
      "Epoch: 565 Train Loss: 0.3308448760645476 Train Accuracy: 88.41250000000001\n",
      "Epoch: 566 Train Loss: 0.3302920617173476 Train Accuracy: 88.25\n",
      "Epoch: 567 Train Loss: 0.33050306205164715 Train Accuracy: 88.43124999999999\n",
      "Epoch: 568 Train Loss: 0.3299222274052699 Train Accuracy: 88.26875\n",
      "Epoch: 569 Train Loss: 0.33016245386906895 Train Accuracy: 88.45625\n",
      "Epoch: 570 Train Loss: 0.32955298082782364 Train Accuracy: 88.3\n",
      "Epoch: 571 Train Loss: 0.3298222676390989 Train Accuracy: 88.47500000000001\n",
      "Epoch: 572 Train Loss: 0.3291838374812141 Train Accuracy: 88.325\n",
      "Epoch: 573 Train Loss: 0.3294821027393217 Train Accuracy: 88.46875\n",
      "Epoch: 574 Train Loss: 0.3288146868954461 Train Accuracy: 88.34375\n",
      "Epoch: 575 Train Loss: 0.32914192738174647 Train Accuracy: 88.49375\n",
      "Epoch: 576 Train Loss: 0.3284457645003398 Train Accuracy: 88.375\n",
      "Epoch: 577 Train Loss: 0.32880204464763463 Train Accuracy: 88.5\n",
      "Epoch: 578 Train Loss: 0.3280776065257398 Train Accuracy: 88.3875\n",
      "Epoch: 579 Train Loss: 0.32846304159859685 Train Accuracy: 88.4875\n",
      "Epoch: 580 Train Loss: 0.32771099125571646 Train Accuracy: 88.40625\n",
      "Epoch: 581 Train Loss: 0.32812572505981347 Train Accuracy: 88.53125\n",
      "Epoch: 582 Train Loss: 0.3273468705594511 Train Accuracy: 88.43124999999999\n",
      "Epoch: 583 Train Loss: 0.3277910481977989 Train Accuracy: 88.55\n",
      "Epoch: 584 Train Loss: 0.3269862957699564 Train Accuracy: 88.41250000000001\n",
      "Epoch: 585 Train Loss: 0.3274600320909593 Train Accuracy: 88.56875\n",
      "Epoch: 586 Train Loss: 0.3266303417331889 Train Accuracy: 88.44999999999999\n",
      "Epoch: 587 Train Loss: 0.3271336861981087 Train Accuracy: 88.575\n",
      "Epoch: 588 Train Loss: 0.326280032327831 Train Accuracy: 88.48125\n",
      "Epoch: 589 Train Loss: 0.32681293110751586 Train Accuracy: 88.6\n",
      "Epoch: 590 Train Loss: 0.32593627011806986 Train Accuracy: 88.49375\n",
      "Epoch: 591 Train Loss: 0.3264985263596428 Train Accuracy: 88.625\n",
      "Epoch: 592 Train Loss: 0.32559977220713704 Train Accuracy: 88.55\n",
      "Epoch: 593 Train Loss: 0.32619100563239306 Train Accuracy: 88.6375\n",
      "Epoch: 594 Train Loss: 0.3252710139391873 Train Accuracy: 88.5625\n",
      "Epoch: 595 Train Loss: 0.32589062127123186 Train Accuracy: 88.6375\n",
      "Epoch: 596 Train Loss: 0.3249501819324592 Train Accuracy: 88.56875\n",
      "Epoch: 597 Train Loss: 0.32559730009162585 Train Accuracy: 88.66875\n",
      "Epoch: 598 Train Loss: 0.3246371380352063 Train Accuracy: 88.56875\n",
      "Epoch: 599 Train Loss: 0.32531061256071836 Train Accuracy: 88.6875\n",
      "Epoch: 600 Train Loss: 0.3243313961246994 Train Accuracy: 88.56875\n",
      "Epoch: 601 Train Loss: 0.325029757787047 Train Accuracy: 88.7\n",
      "Epoch: 602 Train Loss: 0.3240321140949657 Train Accuracy: 88.58749999999999\n",
      "Epoch: 603 Train Loss: 0.32475356704931163 Train Accuracy: 88.74375\n",
      "Epoch: 604 Train Loss: 0.32373810371412065 Train Accuracy: 88.60625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 605 Train Loss: 0.3244805286612725 Train Accuracy: 88.75\n",
      "Epoch: 606 Train Loss: 0.3234478610477413 Train Accuracy: 88.6\n",
      "Epoch: 607 Train Loss: 0.32420883656054883 Train Accuracy: 88.75625000000001\n",
      "Epoch: 608 Train Loss: 0.3231596196055972 Train Accuracy: 88.60625\n",
      "Epoch: 609 Train Loss: 0.3239364639126468 Train Accuracy: 88.75625000000001\n",
      "Epoch: 610 Train Loss: 0.3228714270906414 Train Accuracy: 88.60625\n",
      "Epoch: 611 Train Loss: 0.3236612611205943 Train Accuracy: 88.7625\n",
      "Epoch: 612 Train Loss: 0.32258124454706316 Train Accuracy: 88.61250000000001\n",
      "Epoch: 613 Train Loss: 0.32338107497413904 Train Accuracy: 88.7875\n",
      "Epoch: 614 Train Loss: 0.3222870639416604 Train Accuracy: 88.6\n",
      "Epoch: 615 Train Loss: 0.32309388253017357 Train Accuracy: 88.8\n",
      "Epoch: 616 Train Loss: 0.32198703711916815 Train Accuracy: 88.6\n",
      "Epoch: 617 Train Loss: 0.3227979301847925 Train Accuracy: 88.8\n",
      "Epoch: 618 Train Loss: 0.32167960620348784 Train Accuracy: 88.63125\n",
      "Epoch: 619 Train Loss: 0.32249186593522533 Train Accuracy: 88.83125\n",
      "Epoch: 620 Train Loss: 0.32136362353931247 Train Accuracy: 88.65625\n",
      "Epoch: 621 Train Loss: 0.3221748517123089 Train Accuracy: 88.81875000000001\n",
      "Epoch: 622 Train Loss: 0.32103844879382326 Train Accuracy: 88.69375000000001\n",
      "Epoch: 623 Train Loss: 0.32184664339069097 Train Accuracy: 88.83125\n",
      "Epoch: 624 Train Loss: 0.3207040122302474 Train Accuracy: 88.675\n",
      "Epoch: 625 Train Loss: 0.32150762879595085 Train Accuracy: 88.84375\n",
      "Epoch: 626 Train Loss: 0.32036083639023405 Train Accuracy: 88.70625\n",
      "Epoch: 627 Train Loss: 0.32115881841059796 Train Accuracy: 88.85625\n",
      "Epoch: 628 Train Loss: 0.3200100130048906 Train Accuracy: 88.7375\n",
      "Epoch: 629 Train Loss: 0.3208017888030093 Train Accuracy: 88.85\n",
      "Epoch: 630 Train Loss: 0.31965313707584214 Train Accuracy: 88.74375\n",
      "Epoch: 631 Train Loss: 0.32043858409511017 Train Accuracy: 88.85\n",
      "Epoch: 632 Train Loss: 0.31929220477974335 Train Accuracy: 88.7625\n",
      "Epoch: 633 Train Loss: 0.3200715850990757 Train Accuracy: 88.85\n",
      "Epoch: 634 Train Loss: 0.3189294853332859 Train Accuracy: 88.775\n",
      "Epoch: 635 Train Loss: 0.3197033584169984 Train Accuracy: 88.8625\n",
      "Epoch: 636 Train Loss: 0.31856737872910995 Train Accuracy: 88.79375\n",
      "Epoch: 637 Train Loss: 0.3193364985626801 Train Accuracy: 88.8875\n",
      "Epoch: 638 Train Loss: 0.318208271261157 Train Accuracy: 88.78125\n",
      "Epoch: 639 Train Loss: 0.31897347522057734 Train Accuracy: 88.90625\n",
      "Epoch: 640 Train Loss: 0.317854399326044 Train Accuracy: 88.78125\n",
      "Epoch: 641 Train Loss: 0.3186164956127501 Train Accuracy: 88.91250000000001\n",
      "Epoch: 642 Train Loss: 0.31750772967297664 Train Accuracy: 88.79375\n",
      "Epoch: 643 Train Loss: 0.3182673892437727 Train Accuracy: 88.91250000000001\n",
      "Epoch: 644 Train Loss: 0.31716986168574907 Train Accuracy: 88.8125\n",
      "Epoch: 645 Train Loss: 0.31792751962769267 Train Accuracy: 88.925\n",
      "Epoch: 646 Train Loss: 0.31684195491533174 Train Accuracy: 88.8375\n",
      "Epoch: 647 Train Loss: 0.3175977253784708 Train Accuracy: 88.91250000000001\n",
      "Epoch: 648 Train Loss: 0.31652468323744054 Train Accuracy: 88.8375\n",
      "Epoch: 649 Train Loss: 0.31727829143531616 Train Accuracy: 88.91875\n",
      "Epoch: 650 Train Loss: 0.31621821576025216 Train Accuracy: 88.8375\n",
      "Epoch: 651 Train Loss: 0.3169689501431762 Train Accuracy: 88.98125\n",
      "Epoch: 652 Train Loss: 0.31592222384074503 Train Accuracy: 88.85625\n",
      "Epoch: 653 Train Loss: 0.3166689112046459 Train Accuracy: 88.99375\n",
      "Epoch: 654 Train Loss: 0.315635913056891 Train Accuracy: 88.83125\n",
      "Epoch: 655 Train Loss: 0.31637691888260383 Train Accuracy: 89.00625\n",
      "Epoch: 656 Train Loss: 0.3153580784689248 Train Accuracy: 88.8375\n",
      "Epoch: 657 Train Loss: 0.31609133400713896 Train Accuracy: 89.0125\n",
      "Epoch: 658 Train Loss: 0.3150871807758192 Train Accuracy: 88.84375\n",
      "Epoch: 659 Train Loss: 0.3158102371737235 Train Accuracy: 89.025\n",
      "Epoch: 660 Train Loss: 0.3148214399322255 Train Accuracy: 88.85625\n",
      "Epoch: 661 Train Loss: 0.31553154801725464 Train Accuracy: 89.025\n",
      "Epoch: 662 Train Loss: 0.3145589414807909 Train Accuracy: 88.85625\n",
      "Epoch: 663 Train Loss: 0.3152531537871935 Train Accuracy: 89.05\n",
      "Epoch: 664 Train Loss: 0.31429774946515227 Train Accuracy: 88.86874999999999\n",
      "Epoch: 665 Train Loss: 0.314973038956159 Train Accuracy: 89.0625\n",
      "Epoch: 666 Train Loss: 0.31403601861887354 Train Accuracy: 88.85625\n",
      "Epoch: 667 Train Loss: 0.31468940666329615 Train Accuracy: 89.0625\n",
      "Epoch: 668 Train Loss: 0.3137720979106736 Train Accuracy: 88.86874999999999\n",
      "Epoch: 669 Train Loss: 0.31440078278602535 Train Accuracy: 89.05625\n",
      "Epoch: 670 Train Loss: 0.3135046177463154 Train Accuracy: 88.88125\n",
      "Epoch: 671 Train Loss: 0.31410609456576183 Train Accuracy: 89.06875\n",
      "Epoch: 672 Train Loss: 0.31323255431846536 Train Accuracy: 88.88125\n",
      "Epoch: 673 Train Loss: 0.31380471797287574 Train Accuracy: 89.08749999999999\n",
      "Epoch: 674 Train Loss: 0.31295526669311324 Train Accuracy: 88.91250000000001\n",
      "Epoch: 675 Train Loss: 0.313496491118716 Train Accuracy: 89.08749999999999\n",
      "Epoch: 676 Train Loss: 0.31267250495645593 Train Accuracy: 88.90625\n",
      "Epoch: 677 Train Loss: 0.3131816945382171 Train Accuracy: 89.09375\n",
      "Epoch: 678 Train Loss: 0.31238439070213136 Train Accuracy: 88.9\n",
      "Epoch: 679 Train Loss: 0.31286100250953586 Train Accuracy: 89.08125\n",
      "Epoch: 680 Train Loss: 0.3120913738467477 Train Accuracy: 88.93124999999999\n",
      "Epoch: 681 Train Loss: 0.3125354122215771 Train Accuracy: 89.1125\n",
      "Epoch: 682 Train Loss: 0.3117941718123092 Train Accuracy: 88.94375\n",
      "Epoch: 683 Train Loss: 0.31220615918073946 Train Accuracy: 89.1375\n",
      "Epoch: 684 Train Loss: 0.3114936982458559 Train Accuracy: 88.91250000000001\n",
      "Epoch: 685 Train Loss: 0.3118746276286378 Train Accuracy: 89.1625\n",
      "Epoch: 686 Train Loss: 0.3111909885886641 Train Accuracy: 88.9\n",
      "Epoch: 687 Train Loss: 0.31154226401831153 Train Accuracy: 89.21249999999999\n",
      "Epoch: 688 Train Loss: 0.31088712906981003 Train Accuracy: 88.925\n",
      "Epoch: 689 Train Loss: 0.3112105000392678 Train Accuracy: 89.24375\n",
      "Epoch: 690 Train Loss: 0.3105831943211638 Train Accuracy: 88.925\n",
      "Epoch: 691 Train Loss: 0.31088068965250476 Train Accuracy: 89.24375\n",
      "Epoch: 692 Train Loss: 0.31028019709033694 Train Accuracy: 88.925\n",
      "Epoch: 693 Train Loss: 0.31055406245673317 Train Accuracy: 89.275\n",
      "Epoch: 694 Train Loss: 0.3099790517521101 Train Accuracy: 88.94999999999999\n",
      "Epoch: 695 Train Loss: 0.31023169375114795 Train Accuracy: 89.28125\n",
      "Epoch: 696 Train Loss: 0.30968055171640113 Train Accuracy: 88.9625\n",
      "Epoch: 697 Train Loss: 0.30991449008219746 Train Accuracy: 89.30624999999999\n",
      "Epoch: 698 Train Loss: 0.3093853595496575 Train Accuracy: 88.96875\n",
      "Epoch: 699 Train Loss: 0.30960318795146047 Train Accuracy: 89.3125\n",
      "Epoch: 700 Train Loss: 0.3090940077334941 Train Accuracy: 88.96875\n",
      "Epoch: 701 Train Loss: 0.3092983627224673 Train Accuracy: 89.31875000000001\n",
      "Epoch: 702 Train Loss: 0.3088069074804617 Train Accuracy: 88.97500000000001\n",
      "Epoch: 703 Train Loss: 0.3090004445414136 Train Accuracy: 89.35625\n",
      "Epoch: 704 Train Loss: 0.3085243628687605 Train Accuracy: 88.99375\n",
      "Epoch: 705 Train Loss: 0.30870973819536657 Train Accuracy: 89.38125\n",
      "Epoch: 706 Train Loss: 0.30824658767858765 Train Accuracy: 88.9875\n",
      "Epoch: 707 Train Loss: 0.3084264441787884 Train Accuracy: 89.39375000000001\n",
      "Epoch: 708 Train Loss: 0.3079737226381494 Train Accuracy: 89.0\n",
      "Epoch: 709 Train Loss: 0.30815067873808405 Train Accuracy: 89.425\n",
      "Epoch: 710 Train Loss: 0.3077058512454183 Train Accuracy: 89.0\n",
      "Epoch: 711 Train Loss: 0.30788249124050504 Train Accuracy: 89.44375\n",
      "Epoch: 712 Train Loss: 0.3074430128588596 Train Accuracy: 89.025\n",
      "Epoch: 713 Train Loss: 0.3076218778091731 Train Accuracy: 89.45625\n",
      "Epoch: 714 Train Loss: 0.30718521229310786 Train Accuracy: 89.03125\n",
      "Epoch: 715 Train Loss: 0.3073687907352585 Train Accuracy: 89.47500000000001\n",
      "Epoch: 716 Train Loss: 0.30693242567087775 Train Accuracy: 89.05\n",
      "Epoch: 717 Train Loss: 0.30712314368855476 Train Accuracy: 89.50625\n",
      "Epoch: 718 Train Loss: 0.30668460273705306 Train Accuracy: 89.05625\n",
      "Epoch: 719 Train Loss: 0.30688481317586713 Train Accuracy: 89.51875\n",
      "Epoch: 720 Train Loss: 0.30644166621100793 Train Accuracy: 89.075\n",
      "Epoch: 721 Train Loss: 0.30665363702804094 Train Accuracy: 89.51249999999999\n",
      "Epoch: 722 Train Loss: 0.3062035090233961 Train Accuracy: 89.10625\n",
      "Epoch: 723 Train Loss: 0.3064294109232126 Train Accuracy: 89.51249999999999\n",
      "Epoch: 724 Train Loss: 0.3059699904463726 Train Accuracy: 89.11875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 725 Train Loss: 0.3062118840738008 Train Accuracy: 89.51875\n",
      "Epoch: 726 Train Loss: 0.3057409321808565 Train Accuracy: 89.14999999999999\n",
      "Epoch: 727 Train Loss: 0.3060007552204033 Train Accuracy: 89.51249999999999\n",
      "Epoch: 728 Train Loss: 0.30551611541635104 Train Accuracy: 89.14375\n",
      "Epoch: 729 Train Loss: 0.30579566999367 Train Accuracy: 89.51875\n",
      "Epoch: 730 Train Loss: 0.30529527973835796 Train Accuracy: 89.14375\n",
      "Epoch: 731 Train Loss: 0.3055962205353877 Train Accuracy: 89.51875\n",
      "Epoch: 732 Train Loss: 0.3050781245402278 Train Accuracy: 89.14999999999999\n",
      "Epoch: 733 Train Loss: 0.3054019480259013 Train Accuracy: 89.55\n",
      "Epoch: 734 Train Loss: 0.30486431331880925 Train Accuracy: 89.14375\n",
      "Epoch: 735 Train Loss: 0.30521234846369333 Train Accuracy: 89.54375\n",
      "Epoch: 736 Train Loss: 0.30465348091847044 Train Accuracy: 89.16875\n",
      "Epoch: 737 Train Loss: 0.30502688170557235 Train Accuracy: 89.54375\n",
      "Epoch: 738 Train Loss: 0.30444524346105545 Train Accuracy: 89.1875\n",
      "Epoch: 739 Train Loss: 0.30484498342770394 Train Accuracy: 89.5625\n",
      "Epoch: 740 Train Loss: 0.30423921038757606 Train Accuracy: 89.19375000000001\n",
      "Epoch: 741 Train Loss: 0.3046660793373631 Train Accuracy: 89.58125\n",
      "Epoch: 742 Train Loss: 0.3040349977693685 Train Accuracy: 89.24375\n",
      "Epoch: 743 Train Loss: 0.3044896006834258 Train Accuracy: 89.59375\n",
      "Epoch: 744 Train Loss: 0.3038322418491462 Train Accuracy: 89.25\n",
      "Epoch: 745 Train Loss: 0.3043149999099789 Train Accuracy: 89.59375\n",
      "Epoch: 746 Train Loss: 0.3036306116686733 Train Accuracy: 89.23124999999999\n",
      "Epoch: 747 Train Loss: 0.3041417651972486 Train Accuracy: 89.58125\n",
      "Epoch: 748 Train Loss: 0.3034298196449506 Train Accuracy: 89.25625000000001\n",
      "Epoch: 749 Train Loss: 0.3039694326541741 Train Accuracy: 89.58749999999999\n",
      "Epoch: 750 Train Loss: 0.3032296290758357 Train Accuracy: 89.25\n",
      "Epoch: 751 Train Loss: 0.3037975950724188 Train Accuracy: 89.56875\n",
      "Epoch: 752 Train Loss: 0.3030298577818792 Train Accuracy: 89.2625\n",
      "Epoch: 753 Train Loss: 0.30362590641381804 Train Accuracy: 89.575\n",
      "Epoch: 754 Train Loss: 0.3028303774049044 Train Accuracy: 89.26875\n",
      "Epoch: 755 Train Loss: 0.30345408155999326 Train Accuracy: 89.60000000000001\n",
      "Epoch: 756 Train Loss: 0.3026311082565009 Train Accuracy: 89.29375\n",
      "Epoch: 757 Train Loss: 0.3032818912707096 Train Accuracy: 89.60625\n",
      "Epoch: 758 Train Loss: 0.3024320100057006 Train Accuracy: 89.31875000000001\n",
      "Epoch: 759 Train Loss: 0.30310915273599803 Train Accuracy: 89.61875\n",
      "Epoch: 760 Train Loss: 0.3022330688772525 Train Accuracy: 89.34375\n",
      "Epoch: 761 Train Loss: 0.30293571652368256 Train Accuracy: 89.61875\n",
      "Epoch: 762 Train Loss: 0.3020342823651891 Train Accuracy: 89.3375\n",
      "Epoch: 763 Train Loss: 0.3027614510794088 Train Accuracy: 89.6125\n",
      "Epoch: 764 Train Loss: 0.3018356427219663 Train Accuracy: 89.35\n",
      "Epoch: 765 Train Loss: 0.30258622619837044 Train Accuracy: 89.61875\n",
      "Epoch: 766 Train Loss: 0.3016371206407548 Train Accuracy: 89.3625\n",
      "Epoch: 767 Train Loss: 0.3024098970339185 Train Accuracy: 89.58749999999999\n",
      "Epoch: 768 Train Loss: 0.30143865059550556 Train Accuracy: 89.3625\n",
      "Epoch: 769 Train Loss: 0.3022322902254537 Train Accuracy: 89.61875\n",
      "Epoch: 770 Train Loss: 0.3012401192362478 Train Accuracy: 89.375\n",
      "Epoch: 771 Train Loss: 0.3020531936132761 Train Accuracy: 89.64375\n",
      "Epoch: 772 Train Loss: 0.30104135805830035 Train Accuracy: 89.4\n",
      "Epoch: 773 Train Loss: 0.30187235076648705 Train Accuracy: 89.66875\n",
      "Epoch: 774 Train Loss: 0.30084214128186476 Train Accuracy: 89.39375000000001\n",
      "Epoch: 775 Train Loss: 0.3016894611941804 Train Accuracy: 89.66875\n",
      "Epoch: 776 Train Loss: 0.30064218950619526 Train Accuracy: 89.41875\n",
      "Epoch: 777 Train Loss: 0.3015041866598226 Train Accuracy: 89.6625\n",
      "Epoch: 778 Train Loss: 0.3004411792595 Train Accuracy: 89.41875\n",
      "Epoch: 779 Train Loss: 0.3013161635015844 Train Accuracy: 89.66875\n",
      "Epoch: 780 Train Loss: 0.3002387580783734 Train Accuracy: 89.45625\n",
      "Epoch: 781 Train Loss: 0.3011250203137204 Train Accuracy: 89.68125\n",
      "Epoch: 782 Train Loss: 0.3000345642535325 Train Accuracy: 89.46875\n",
      "Epoch: 783 Train Loss: 0.30093039981071207 Train Accuracy: 89.68125\n",
      "Epoch: 784 Train Loss: 0.29982824991443296 Train Accuracy: 89.46875\n",
      "Epoch: 785 Train Loss: 0.30073198322824696 Train Accuracy: 89.6875\n",
      "Epoch: 786 Train Loss: 0.29961950574183294 Train Accuracy: 89.47500000000001\n",
      "Epoch: 787 Train Loss: 0.3005295152667896 Train Accuracy: 89.6875\n",
      "Epoch: 788 Train Loss: 0.29940808534250946 Train Accuracy: 89.48125\n",
      "Epoch: 789 Train Loss: 0.3003228274032129 Train Accuracy: 89.6875\n",
      "Epoch: 790 Train Loss: 0.29919382723475224 Train Accuracy: 89.5\n",
      "Epoch: 791 Train Loss: 0.3001118574189597 Train Accuracy: 89.67500000000001\n",
      "Epoch: 792 Train Loss: 0.2989766725020372 Train Accuracy: 89.5\n",
      "Epoch: 793 Train Loss: 0.29989666323299247 Train Accuracy: 89.6875\n",
      "Epoch: 794 Train Loss: 0.29875667647786736 Train Accuracy: 89.5\n",
      "Epoch: 795 Train Loss: 0.2996774295705723 Train Accuracy: 89.7\n",
      "Epoch: 796 Train Loss: 0.29853401330328755 Train Accuracy: 89.5\n",
      "Epoch: 797 Train Loss: 0.29945446660254743 Train Accuracy: 89.7\n",
      "Epoch: 798 Train Loss: 0.29830897280118135 Train Accuracy: 89.53750000000001\n",
      "Epoch: 799 Train Loss: 0.2992282003885515 Train Accuracy: 89.70625\n",
      "Epoch: 800 Train Loss: 0.29808194977074226 Train Accuracy: 89.53125\n",
      "Epoch: 801 Train Loss: 0.29899915567167434 Train Accuracy: 89.70625\n",
      "Epoch: 802 Train Loss: 0.29785342644626384 Train Accuracy: 89.53125\n",
      "Epoch: 803 Train Loss: 0.29876793222111137 Train Accuracy: 89.71249999999999\n",
      "Epoch: 804 Train Loss: 0.2976239494158775 Train Accuracy: 89.53750000000001\n",
      "Epoch: 805 Train Loss: 0.2985351764343721 Train Accuracy: 89.725\n",
      "Epoch: 806 Train Loss: 0.29739410270306055 Train Accuracy: 89.54375\n",
      "Epoch: 807 Train Loss: 0.29830155024523297 Train Accuracy: 89.71875\n",
      "Epoch: 808 Train Loss: 0.29716447894476206 Train Accuracy: 89.56875\n",
      "Epoch: 809 Train Loss: 0.2980676995183438 Train Accuracy: 89.74375\n",
      "Epoch: 810 Train Loss: 0.2969356506484683 Train Accuracy: 89.60000000000001\n",
      "Epoch: 811 Train Loss: 0.29783422405334775 Train Accuracy: 89.75\n",
      "Epoch: 812 Train Loss: 0.29670814339283025 Train Accuracy: 89.60000000000001\n",
      "Epoch: 813 Train Loss: 0.29760165109877984 Train Accuracy: 89.7625\n",
      "Epoch: 814 Train Loss: 0.29648241258521923 Train Accuracy: 89.63125\n",
      "Epoch: 815 Train Loss: 0.297370413929931 Train Accuracy: 89.76875\n",
      "Epoch: 816 Train Loss: 0.2962588250454851 Train Accuracy: 89.61875\n",
      "Epoch: 817 Train Loss: 0.2971408366204655 Train Accuracy: 89.775\n",
      "Epoch: 818 Train Loss: 0.29603764628939294 Train Accuracy: 89.6375\n",
      "Epoch: 819 Train Loss: 0.2969131256769395 Train Accuracy: 89.8\n",
      "Epoch: 820 Train Loss: 0.29581903397354853 Train Accuracy: 89.65625\n",
      "Epoch: 821 Train Loss: 0.2966873687431796 Train Accuracy: 89.7875\n",
      "Epoch: 822 Train Loss: 0.29560303756390055 Train Accuracy: 89.66875\n",
      "Epoch: 823 Train Loss: 0.2964635401436663 Train Accuracy: 89.825\n",
      "Epoch: 824 Train Loss: 0.2953896039216191 Train Accuracy: 89.6625\n",
      "Epoch: 825 Train Loss: 0.29624151264010123 Train Accuracy: 89.85\n",
      "Epoch: 826 Train Loss: 0.2951785881763233 Train Accuracy: 89.6625\n",
      "Epoch: 827 Train Loss: 0.2960210744365864 Train Accuracy: 89.85\n",
      "Epoch: 828 Train Loss: 0.29496976898635 Train Accuracy: 89.66875\n",
      "Epoch: 829 Train Loss: 0.29580195019742755 Train Accuracy: 89.84375\n",
      "Epoch: 830 Train Loss: 0.294762867076519 Train Accuracy: 89.6625\n",
      "Epoch: 831 Train Loss: 0.29558382464788224 Train Accuracy: 89.85\n",
      "Epoch: 832 Train Loss: 0.294557565802906 Train Accuracy: 89.64999999999999\n",
      "Epoch: 833 Train Loss: 0.2953663672222276 Train Accuracy: 89.8625\n",
      "Epoch: 834 Train Loss: 0.2943535324284107 Train Accuracy: 89.65625\n",
      "Epoch: 835 Train Loss: 0.29514925621356297 Train Accuracy: 89.89375\n",
      "Epoch: 836 Train Loss: 0.2941504388076988 Train Accuracy: 89.66875\n",
      "Epoch: 837 Train Loss: 0.29493220096982103 Train Accuracy: 89.90625\n",
      "Epoch: 838 Train Loss: 0.29394798027698604 Train Accuracy: 89.66875\n",
      "Epoch: 839 Train Loss: 0.29471496086775756 Train Accuracy: 89.91875\n",
      "Epoch: 840 Train Loss: 0.29374589171889914 Train Accuracy: 89.6875\n",
      "Epoch: 841 Train Loss: 0.29449736006959126 Train Accuracy: 89.9125\n",
      "Epoch: 842 Train Loss: 0.29354396001384303 Train Accuracy: 89.7\n",
      "Epoch: 843 Train Loss: 0.29427929740456027 Train Accuracy: 89.9375\n",
      "Epoch: 844 Train Loss: 0.29334203237828416 Train Accuracy: 89.6875\n",
      "Epoch: 845 Train Loss: 0.2940607510911068 Train Accuracy: 89.9125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 846 Train Loss: 0.29314002040260245 Train Accuracy: 89.69375000000001\n",
      "Epoch: 847 Train Loss: 0.2938417783911489 Train Accuracy: 89.9125\n",
      "Epoch: 848 Train Loss: 0.29293789990917785 Train Accuracy: 89.69375000000001\n",
      "Epoch: 849 Train Loss: 0.2936225106320147 Train Accuracy: 89.90625\n",
      "Epoch: 850 Train Loss: 0.29273570702818963 Train Accuracy: 89.7\n",
      "Epoch: 851 Train Loss: 0.29340314431443026 Train Accuracy: 89.9125\n",
      "Epoch: 852 Train Loss: 0.2925335311113902 Train Accuracy: 89.725\n",
      "Epoch: 853 Train Loss: 0.29318392922501835 Train Accuracy: 89.90625\n",
      "Epoch: 854 Train Loss: 0.29233150525710677 Train Accuracy: 89.7375\n",
      "Epoch: 855 Train Loss: 0.2929651545782721 Train Accuracy: 89.91875\n",
      "Epoch: 856 Train Loss: 0.2921297952958566 Train Accuracy: 89.74375\n",
      "Epoch: 857 Train Loss: 0.29274713422636023 Train Accuracy: 89.95625000000001\n",
      "Epoch: 858 Train Loss: 0.2919285880870283 Train Accuracy: 89.75\n",
      "Epoch: 859 Train Loss: 0.2925301919058583 Train Accuracy: 89.96875\n",
      "Epoch: 860 Train Loss: 0.2917280799125708 Train Accuracy: 89.74375\n",
      "Epoch: 861 Train Loss: 0.29231464735644697 Train Accuracy: 89.99374999999999\n",
      "Epoch: 862 Train Loss: 0.29152846563851936 Train Accuracy: 89.75625000000001\n",
      "Epoch: 863 Train Loss: 0.29210080396971944 Train Accuracy: 90.00625\n",
      "Epoch: 864 Train Loss: 0.2913299291673132 Train Accuracy: 89.74375\n",
      "Epoch: 865 Train Loss: 0.291888938429147 Train Accuracy: 90.01875\n",
      "Epoch: 866 Train Loss: 0.2911326355413203 Train Accuracy: 89.78125\n",
      "Epoch: 867 Train Loss: 0.29167929260552944 Train Accuracy: 90.01875\n",
      "Epoch: 868 Train Loss: 0.290936724897045 Train Accuracy: 89.79375\n",
      "Epoch: 869 Train Loss: 0.2914720677923714 Train Accuracy: 90.03125\n",
      "Epoch: 870 Train Loss: 0.29074230832308934 Train Accuracy: 89.80625\n",
      "Epoch: 871 Train Loss: 0.2912674212140394 Train Accuracy: 90.03125\n",
      "Epoch: 872 Train Loss: 0.29054946555202743 Train Accuracy: 89.81875000000001\n",
      "Epoch: 873 Train Loss: 0.29106546462256333 Train Accuracy: 90.05625\n",
      "Epoch: 874 Train Loss: 0.29035824432185614 Train Accuracy: 89.80625\n",
      "Epoch: 875 Train Loss: 0.29086626471825 Train Accuracy: 90.06875\n",
      "Epoch: 876 Train Loss: 0.29016866117791357 Train Accuracy: 89.81875000000001\n",
      "Epoch: 877 Train Loss: 0.2906698450829962 Train Accuracy: 90.05625\n",
      "Epoch: 878 Train Loss: 0.28998070344956856 Train Accuracy: 89.825\n",
      "Epoch: 879 Train Loss: 0.29047618929883706 Train Accuracy: 90.0625\n",
      "Epoch: 880 Train Loss: 0.28979433212402933 Train Accuracy: 89.825\n",
      "Epoch: 881 Train Loss: 0.29028524493198826 Train Accuracy: 90.05\n",
      "Epoch: 882 Train Loss: 0.28960948534752146 Train Accuracy: 89.85\n",
      "Epoch: 883 Train Loss: 0.2900969280879749 Train Accuracy: 90.05\n",
      "Epoch: 884 Train Loss: 0.28942608230663547 Train Accuracy: 89.85625\n",
      "Epoch: 885 Train Loss: 0.2899111282801447 Train Accuracy: 90.0625\n",
      "Epoch: 886 Train Loss: 0.28924402727468496 Train Accuracy: 89.875\n",
      "Epoch: 887 Train Loss: 0.28972771339632486 Train Accuracy: 90.075\n",
      "Epoch: 888 Train Loss: 0.28906321364481075 Train Accuracy: 89.88125\n",
      "Epoch: 889 Train Loss: 0.2895465345920009 Train Accuracy: 90.08125\n",
      "Epoch: 890 Train Loss: 0.2888835278094386 Train Accuracy: 89.8875\n",
      "Epoch: 891 Train Loss: 0.28936743097962164 Train Accuracy: 90.09375\n",
      "Epoch: 892 Train Loss: 0.2887048527815681 Train Accuracy: 89.9\n",
      "Epoch: 893 Train Loss: 0.28919023402010735 Train Accuracy: 90.10625\n",
      "Epoch: 894 Train Loss: 0.2885270714851986 Train Accuracy: 89.90625\n",
      "Epoch: 895 Train Loss: 0.28901477155299227 Train Accuracy: 90.11875\n",
      "Epoch: 896 Train Loss: 0.28835006966883137 Train Accuracy: 89.9125\n",
      "Epoch: 897 Train Loss: 0.2888408714254614 Train Accuracy: 90.1125\n",
      "Epoch: 898 Train Loss: 0.28817373841706034 Train Accuracy: 89.90625\n",
      "Epoch: 899 Train Loss: 0.2886683646981543 Train Accuracy: 90.125\n",
      "Epoch: 900 Train Loss: 0.2879979762510292 Train Accuracy: 89.9125\n",
      "Epoch: 901 Train Loss: 0.2884970884178554 Train Accuracy: 90.1125\n",
      "Epoch: 902 Train Loss: 0.2878226908197114 Train Accuracy: 89.9125\n",
      "Epoch: 903 Train Loss: 0.2883268879552589 Train Accuracy: 90.11875\n",
      "Epoch: 904 Train Loss: 0.2876478001915499 Train Accuracy: 89.91875\n",
      "Epoch: 905 Train Loss: 0.2881576189112036 Train Accuracy: 90.1125\n",
      "Epoch: 906 Train Loss: 0.2874732337610595 Train Accuracy: 89.9125\n",
      "Epoch: 907 Train Loss: 0.28798914859838715 Train Accuracy: 90.125\n",
      "Epoch: 908 Train Loss: 0.2872989327885903 Train Accuracy: 89.91875\n",
      "Epoch: 909 Train Loss: 0.2878213571086972 Train Accuracy: 90.13125\n",
      "Epoch: 910 Train Loss: 0.28712485059443343 Train Accuracy: 89.91875\n",
      "Epoch: 911 Train Loss: 0.2876541379797499 Train Accuracy: 90.13125\n",
      "Epoch: 912 Train Loss: 0.2869509524314555 Train Accuracy: 89.925\n",
      "Epoch: 913 Train Loss: 0.287487398478492 Train Accuracy: 90.14375\n",
      "Epoch: 914 Train Loss: 0.2867772150638184 Train Accuracy: 89.91875\n",
      "Epoch: 915 Train Loss: 0.28732105952496273 Train Accuracy: 90.14375\n",
      "Epoch: 916 Train Loss: 0.2866036260831478 Train Accuracy: 89.9125\n",
      "Epoch: 917 Train Loss: 0.28715505528537716 Train Accuracy: 90.17500000000001\n",
      "Epoch: 918 Train Loss: 0.28643018299761425 Train Accuracy: 89.91875\n",
      "Epoch: 919 Train Loss: 0.28698933247019703 Train Accuracy: 90.17500000000001\n",
      "Epoch: 920 Train Loss: 0.2862568921334515 Train Accuracy: 89.93124999999999\n",
      "Epoch: 921 Train Loss: 0.2868238493792773 Train Accuracy: 90.17500000000001\n",
      "Epoch: 922 Train Loss: 0.2860837673920491 Train Accuracy: 89.95625000000001\n",
      "Epoch: 923 Train Loss: 0.2866585747419041 Train Accuracy: 90.18125\n",
      "Epoch: 924 Train Loss: 0.2859108289084774 Train Accuracy: 89.96875\n",
      "Epoch: 925 Train Loss: 0.2864934864040172 Train Accuracy: 90.18125\n",
      "Epoch: 926 Train Loss: 0.2857381016587578 Train Accuracy: 89.96875\n",
      "Epoch: 927 Train Loss: 0.28632856991764133 Train Accuracy: 90.1875\n",
      "Epoch: 928 Train Loss: 0.2855656140630787 Train Accuracy: 89.98125\n",
      "Epoch: 929 Train Loss: 0.28616381708822564 Train Accuracy: 90.20625\n",
      "Epoch: 930 Train Loss: 0.28539339663033847 Train Accuracy: 90.0\n",
      "Epoch: 931 Train Loss: 0.2859992245340355 Train Accuracy: 90.225\n",
      "Epoch: 932 Train Loss: 0.2852214806858485 Train Accuracy: 89.99374999999999\n",
      "Epoch: 933 Train Loss: 0.2858347923079994 Train Accuracy: 90.23125\n",
      "Epoch: 934 Train Loss: 0.2850498972188997 Train Accuracy: 90.01875\n",
      "Epoch: 935 Train Loss: 0.2856705226266678 Train Accuracy: 90.21249999999999\n",
      "Epoch: 936 Train Loss: 0.2848786758804137 Train Accuracy: 90.03750000000001\n",
      "Epoch: 937 Train Loss: 0.2855064187435372 Train Accuracy: 90.21875\n",
      "Epoch: 938 Train Loss: 0.2847078441534369 Train Accuracy: 90.05625\n",
      "Epoch: 939 Train Loss: 0.28534248399535245 Train Accuracy: 90.25\n",
      "Epoch: 940 Train Loss: 0.2845374267111552 Train Accuracy: 90.0625\n",
      "Epoch: 941 Train Loss: 0.28517872104063496 Train Accuracy: 90.2625\n",
      "Epoch: 942 Train Loss: 0.2843674449688596 Train Accuracy: 90.075\n",
      "Epoch: 943 Train Loss: 0.28501513130008416 Train Accuracy: 90.275\n",
      "Epoch: 944 Train Loss: 0.2841979168282634 Train Accuracy: 90.08125\n",
      "Epoch: 945 Train Loss: 0.2848517145991691 Train Accuracy: 90.26875\n",
      "Epoch: 946 Train Loss: 0.28402885660513694 Train Accuracy: 90.08749999999999\n",
      "Epoch: 947 Train Loss: 0.28468846900459666 Train Accuracy: 90.275\n",
      "Epoch: 948 Train Loss: 0.2838602751246962 Train Accuracy: 90.09375\n",
      "Epoch: 949 Train Loss: 0.2845253908387784 Train Accuracy: 90.28125\n",
      "Epoch: 950 Train Loss: 0.2836921799637964 Train Accuracy: 90.09375\n",
      "Epoch: 951 Train Loss: 0.2843624748501996 Train Accuracy: 90.29374999999999\n",
      "Epoch: 952 Train Loss: 0.28352457581489376 Train Accuracy: 90.125\n",
      "Epoch: 953 Train Loss: 0.2841997145128926 Train Accuracy: 90.28125\n",
      "Epoch: 954 Train Loss: 0.28335746494404196 Train Accuracy: 90.11875\n",
      "Epoch: 955 Train Loss: 0.2840371024251509 Train Accuracy: 90.2875\n",
      "Epoch: 956 Train Loss: 0.2831908477138827 Train Accuracy: 90.125\n",
      "Epoch: 957 Train Loss: 0.28387463077614017 Train Accuracy: 90.29374999999999\n",
      "Epoch: 958 Train Loss: 0.2830247231426115 Train Accuracy: 90.14375\n",
      "Epoch: 959 Train Loss: 0.28371229184915386 Train Accuracy: 90.2875\n",
      "Epoch: 960 Train Loss: 0.28285908947114285 Train Accuracy: 90.1375\n",
      "Epoch: 961 Train Loss: 0.2835500785317164 Train Accuracy: 90.3\n",
      "Epoch: 962 Train Loss: 0.28269394471298626 Train Accuracy: 90.14375\n",
      "Epoch: 963 Train Loss: 0.2833879848054066 Train Accuracy: 90.29374999999999\n",
      "Epoch: 964 Train Loss: 0.2825292871644946 Train Accuracy: 90.14999999999999\n",
      "Epoch: 965 Train Loss: 0.2832260061918942 Train Accuracy: 90.3\n",
      "Epoch: 966 Train Loss: 0.2823651158569294 Train Accuracy: 90.15625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 967 Train Loss: 0.2830641401360058 Train Accuracy: 90.30625\n",
      "Epoch: 968 Train Loss: 0.2822014309359867 Train Accuracy: 90.1625\n",
      "Epoch: 969 Train Loss: 0.28290238631139675 Train Accuracy: 90.29374999999999\n",
      "Epoch: 970 Train Loss: 0.2820382339588157 Train Accuracy: 90.17500000000001\n",
      "Epoch: 971 Train Loss: 0.2827407468393376 Train Accuracy: 90.29374999999999\n",
      "Epoch: 972 Train Loss: 0.2818755281029306 Train Accuracy: 90.1875\n",
      "Epoch: 973 Train Loss: 0.2825792264159845 Train Accuracy: 90.275\n",
      "Epoch: 974 Train Loss: 0.2817133182855731 Train Accuracy: 90.1875\n",
      "Epoch: 975 Train Loss: 0.282417832348081 Train Accuracy: 90.275\n",
      "Epoch: 976 Train Loss: 0.2815516111958722 Train Accuracy: 90.2\n",
      "Epoch: 977 Train Loss: 0.2822565745011426 Train Accuracy: 90.275\n",
      "Epoch: 978 Train Loss: 0.28139041524543495 Train Accuracy: 90.20625\n",
      "Epoch: 979 Train Loss: 0.28209546516767214 Train Accuracy: 90.2625\n",
      "Epoch: 980 Train Loss: 0.2812297404456986 Train Accuracy: 90.21875\n",
      "Epoch: 981 Train Loss: 0.2819345188657435 Train Accuracy: 90.2875\n",
      "Epoch: 982 Train Loss: 0.2810695982224252 Train Accuracy: 90.225\n",
      "Epoch: 983 Train Loss: 0.2817737520803135 Train Accuracy: 90.3\n",
      "Epoch: 984 Train Loss: 0.2809100011790989 Train Accuracy: 90.23125\n",
      "Epoch: 985 Train Loss: 0.28161318296087584 Train Accuracy: 90.3\n",
      "Epoch: 986 Train Loss: 0.2807509628217134 Train Accuracy: 90.24375\n",
      "Epoch: 987 Train Loss: 0.2814528309895692 Train Accuracy: 90.30625\n",
      "Epoch: 988 Train Loss: 0.2805924972575486 Train Accuracy: 90.24375\n",
      "Epoch: 989 Train Loss: 0.28129271663366207 Train Accuracy: 90.325\n",
      "Epoch: 990 Train Loss: 0.2804346188800973 Train Accuracy: 90.24375\n",
      "Epoch: 991 Train Loss: 0.281132860995544 Train Accuracy: 90.33125\n",
      "Epoch: 992 Train Loss: 0.2802773420514044 Train Accuracy: 90.25625000000001\n",
      "Epoch: 993 Train Loss: 0.2809732854720533 Train Accuracy: 90.3375\n",
      "Epoch: 994 Train Loss: 0.2801206807918091 Train Accuracy: 90.2625\n",
      "Epoch: 995 Train Loss: 0.280814011433294 Train Accuracy: 90.35625\n",
      "Epoch: 996 Train Loss: 0.2799646484855423 Train Accuracy: 90.29374999999999\n",
      "Epoch: 997 Train Loss: 0.28065505992915235 Train Accuracy: 90.3625\n",
      "Epoch: 998 Train Loss: 0.27980925760894265 Train Accuracy: 90.30625\n",
      "Epoch: 999 Train Loss: 0.28049645142964214 Train Accuracy: 90.35625\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "clf.fit(trainData,trainLabel,lr=0.0001,epochs = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "daf3be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confn(self,y,t):\n",
    "    return np.mean(-np.sum(t*np.log(y),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d4ee7c22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m clf\u001b[38;5;241m.\u001b[39mplot_loss_graph(\u001b[43mlosses\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "clf.plot_loss_graph(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a20232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self,X):\n",
    "    ypred=self.forward(X)\n",
    "    maxLabel=np.argmax"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
